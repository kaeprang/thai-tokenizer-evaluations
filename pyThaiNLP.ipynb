{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PyThaiNLP is a python module similar to nltk , but it's working primarily on Thai language instead of English. It supports both Python 2.7 and Python 3.\n",
    "PyThaiNLP is a Python library for natural language processing (NLP) of Thai language. Wannaphong Phatthiyaphaibun\n",
    "\n",
    "\n",
    "The chosen engines are 'newmm' which is the default engine of pyThaiNLP and 'deepcut' which is deemed to achieve the highest accuracy.\n",
    "pythainlp.tokenize\n",
    "The pythainlp.tokenize contains multiple functions for tokenizing a chunk of Thai text into desirable units.\n",
    "\n",
    "Here's is my to-do methods:\n",
    "\n",
    "-- Word Tokenization\n",
    "  - Tokenize original raw files with 'newmm' and 'deepcut' engines \n",
    "  - Compare the number of tokenized words by the engines against the MasterCut (based on my judgement) files -- to check accuracy\n",
    "  - newmm (default) - dictionary-based, Maximum Matching + Thai Character Cluster. Dictionary-based Thai Word Segmentation using maximal matching algorithm and Thai Character Cluster (TCC)\n",
    "longest - dictionary-based, Longest Matching\n",
    "deepcut - wrapper for deepcut, language-model-based. Wrapper for deepcut Thai word segmentation. deepcut is a Thai word segmentation library using Deep Neural, specifically, 1D Convolution Neural Network.\n",
    "icu - wrapper for ICU (International Components for Unicode, using PyICU), dictionary-based\n",
    "ulmfit - for thai2fit\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# I try to run the correct function but it seems like typos are still there.\n",
    "# eg x[7] is ['ครอสอาหาร'] should have been  ['คอร์สอาหาร'].\n",
    "\n",
    "from pythainlp.tokenize import word_tokenize\n",
    "from pythainlp import correct\n",
    "\n",
    "sent = 'อิคนนี้เลยค่ะจ้องจะขายครอสอาหารเสริมมังสาวิรัติแพงๆล่าสุดคนแชร์คลิปปปปปป'\n",
    "word_sent = word_tokenize(sent, engine = 'newmm') # or engine = 'deepcut'\n",
    "x = [correct(i) for i in word_sent]\n",
    "print (x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " \n",
    "-- Sentence Tokenization\n",
    "  - Tokenize original raw files with 'newmm' and 'deepcut' engines \n",
    "  - Compare the number of tokenized sentence against the MasterCut files -- to check accuracy\n",
    "  Q: Should I do the sentence tokenization in a another file? Now I use '/' to segment a word, should I use a different symbol for tokenizing sentences in the same file?\n",
    " \n",
    "-- Normalize the raw files and compare against the MasterCut files (Need more research in Thai!)\n",
    "  - Compare the normalized text against the MasterCut version\n",
    "  - Check and compare the speed of the two engines\n",
    "Q: In the files in MasterCut folder, should I clean the texts (remove special chars, punctuations, emojis etc), correct spellings? So that I could use the 'def get_master_count' for word counts, sent counts, normalizing etc.\n",
    "  \n",
    "-- Time speedtest (i don't get the code right yet)\n",
    "\n",
    "ref: \n",
    "pyThaiNLP https://www.thainlp.org/pythainlp/tutorials/notebooks/pythainlp_get_started.html?highlight=normalization#Normalization\n",
    "\n",
    "https://github.com/PyThaiNLP/pythainlp\n",
    "\n",
    "Deepcut\n",
    "https://github.com/rkcosmos/deepcut"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Master Word Count:  3987\n",
      "Master Sent Count:  371\n",
      "เดอะ พร็อพส์ กว่า 15 ปี บนเส้นทางตัวแทนเช่า-ซื้ออสังหาริมทรัพย์ สร้างเมืองไทยเป็นที่อยู่ในฝันของ Expat เอลิสต์ทั่วโลกการติดอันดับเมืองน่าอยู่ของ “กรุงเทพฯ” จากนิตยสารหรือองค์กรการจัดอันดับระดับนานาชาติหลายแห่งนั้น ไม่ใช่ตำแหน่งที่หาซื้อมาได้ด้วยเงินทองหรือใช้อิทธิพลใด ๆ หากแต่เป็นความจริงที่ชาวต่างชาติผู้เคยมาเยือน หรือพักอาศัยใช้ชีวิตอยู่ซึ่งมักถูกเรียกสั้น ๆ ว่า ‘Expat’ ที่มาจากคำเต็มว่า “Expatriate” ต่างยอมรับ และยังคงมองว่าเป็นเมืองน่าอยู่เสมอไม่ใช่เพียงเพราะความพร้อมของที่พักอาศัยที่มีให้เลือกหลากรูปแบบตามความต้องการและงบประมาณที่ได้รับ หรือสิ่งอำนวยความสะดวกต่าง ๆ ที่กรุงเทพฯ พร้อมจะตอบโจทย์ให้กับกลุ่ม Expat ได้มากกว่าประเทศอื่น ๆ ในแถบเอเชีย แต่เบื้องหลังสิ่งที่ทำให้ชาวต่างชาติกลุ่มนี้มั่นใจและเชื่อมั่นก็คือ ตัวแทนเช่า-ซื้ออสังหาริมทรัพย์ระดับอินเตอร์เนชั่นแนลอย่าง คุณณัฐชานันท์ จันทรานุวัฒน์ ผู้ก่อตั้งและซีอีโอของ บริษัท เดอะ พร็อพส์ จำกัด หนึ่งในผู้ที่ทำให้กรุงเทพฯ เป็นเมืองในฝันของเหล่า ‘Expat’ ระดับเอลิสต์ มาตลอดระยะเวลาเกือบ 15 ปี ที่ผ่านมา“ย้อนไป 14 - 15 ปีก่อน Real Estate Agent ยังเป็นเรื่องใหม่มากสำหรับคนไทย เราเองเริ่มรู้จักอาชีพตัวแทนขายอสังหาริมทรัพย์จากรายการโทรทัศน์ ตอนเรียนอยู่ที่อเมริกา แล้วรู้สึกว่าชอบมาก พอเรียนจบกลับมาที่เมืองไทย ก็ไปสมัครงานด้านนี้กับบริษัทที่มีอยู่ในช่วงนั้น เจ้าของเป็นชาวอังกฤษที่มาจากองค์กรระดับอินเตอร์เนชั่นแนลขนาดใหญ่ มีความเข้าใจความเป็นเอเชีย และมีรูปแบบการสอนงานแบบตัวต่อตัว ทำให้เราสามารถเรียนรู้งานได้อย่างรวดเร็ว เริ่มจากทำเรื่องเช่ากับลูกค้าต่างชาติก่อน ซึ่งเราชอบอยู่แล้ว และมีความคุ้นเคยกับวัฒนธรรมของพวกเขา ทำให้เรารู้ว่า Expat ที่เข้ามาอยู่ที่เมืองไทยเขามีความชอบแตกต่างกันยังไง หลังจากนั้นก็ออกมาเปิดบริษัทของตัวเองค่ะ”คุณณัฐชานันท์ เผยจุดเริ่มต้นก่อนมาเปิดบริษัท เดอะ พร็อพส์ เป็นของตัวเอง จากนั้นบริษัทก็ได้สะสมชื่อเสียง ทำหน้าที่ Service & Solution ด้านอสังหาริมทรัพย์ อย่างครบวงจร ไม่ว่าจะเช่า ซื้อ หรือแม้เรื่องการลงทุน จนเป็นที่ไว้เนื้อเชื่อใจของเหล่า Expat โดยเฉพาะกลุ่มเอลิสต์ อย่างเอกอัครราชทูตจากประเทศต่าง ๆ ซึ่งเธอเล่าต่อถึงแนวทางในการดำเนินธุรกิจ จนประสบความสำเร็จอย่างเช่นทุกวันนี้ว่า“สิ่งที่ทำให้เราได้รับความไว้วางใจจาก Expat กลุ่มนี้มาโดยตลอด คือ การบริการที่เราโฟกัสมาก ๆ เพราะเรารักในอาชีพนี้ในฐานะผู้ก่อตั้งบริษัท จึงมีความตั้งใจในการให้บริการกับลูกค้าอย่างดีมาตั้งแต่วินาทีแรกที่ทำงาน สมมติว่าลูกค้าต้องการเช่าอสังหาริมทรัพย์สักแห่ง เราต้องจัดเตรียมข้อมูลทุกอย่าง ดูแลลูกค้าตั้งแต่เข้ามาถึงเมืองไทย จนถึงการพาไปเลือกที่พักระยะยาว การแนะนำโรงเรียนให้กับลูก ๆ ในกรณีที่เขาพาครอบครัวมาด้วย การเตรียมข้อมูลถึงสถานที่อำนวยความสะดวกบริเวณโดยรอบที่พักอาศัย ทั้งแหล่งช็อปปิ้ง สถานที่ทำกิจกรรมของครอบครัวตามไลฟ์สไตล์ เรื่องการท่องเที่ยว ตลอดจนช่วยหาแม่บ้าน และคนขับรถ อย่างน้อยเราต้องส่งลิสต์ให้เขารับรู้ก่อนไม่น้อยกว่า 3 แห่งขั้นต่ำ ตามความต้องการ และทีมงานทุกคนสามารถเห็นความต้องการของลูกค้าทุกราย ผ่านดาต้าขนาดใหญ่ส่วนกลางของบริษัท ทำให้ลูกค้าสามารถได้รับการดูแลเอาใจใส่อย่างทันท่วงทีแม้ในกรณีเกิดปัญหาติดขัดต่างๆ”ไม่ใช่เพียงแค่การเสาะแสวงหาคอนโดมิเนียม หรืออพาร์ตเมนต์ที่ตอบโจทย์กับกลุ่มลูกค้าหลักเท่านั้น แต่ เดอะ พร็อพส์ ภายใต้การนำของคุณณัฐชานันท์ ยังขยายต่อไปถึงการซื้อขายสำหรับชาวต่างชาติที่ต้องการมากกว่าแค่การมาพำนักในห้วงเวลาหนึ่ง“ลูกค้าบางคนมาอยู่แล้วก็มองข้ามไปถึงเรื่องการลงทุนด้วย ซึ่ง เดอะ พร็อพส์ ก็พร้อมบริการอย่างมืออาชีพในมาตรฐานสากล สำหรับกรณีซื้อลงทุนหลังจากลูกค้าตกลงซื้อเป็นที่เรียบร้อยแล้ว เราจะทำหน้าที่บริหารจัดการให้หมด ตั้งแต่ หาผู้เช่า เก็บค่าเช่าส่งไปให้ยังต่างประเทศ จัดการซ่อมแซมห้องหลังผู้เช่าออก รับคำร้องเรียนต่าง ๆ เสมือนเราเป็นแลนด์ลอร์ดตัวเอง อย่างลูกค้าบางราย ซื้ออสังหาริมทรัพย์ต่าง ๆ เอาไว้แล้ว ก็ไม่กลับมาเมืองไทยอีกนานหลายปี ปล่อยให้เราดำเนินการทุกอย่างเลย หรือบางรายให้เราทำการตลาดแล้วขาย สิ่งเหล่านี้ล้วนเกิดจากความไว้วางใจที่สะสมมาตลอดการทำธุรกิจด้านนี้ค่ะ”นอกเหนืองานบริการ และความไว้เนื้อเชื่อใจตามที่กล่าวมาแล้ว อีกส่วนสำคัญ นั่นคือเรื่องความเป็นมืออาชีพด้านภาษา และความเข้าใจทั้งในเชิงธุรกิจ การลงทุน และกฎหมายเกี่ยวข้องกับอสังหาริมทรัพย์ต่าง ๆ ในเมืองไทย เดอะ พร็อพส์ จึงครองใจไม่เพียงแต่ Expat ระดับเอลิสต์เท่านั้น ยังต่อเนื่องไปถึงดีเวลลอปเปอร์ต่าง ๆ เจ้าของที่ดินและนักลงทุนในเมืองไทย ซึ่งมักเชิญเธอไปให้คำปรึกษา ในฐานะผู้เชี่ยวชาญด้านอสังหาริมทรัพย์อย่างครอบคลุมอีกด้วยอย่างไรก็ตามแม้กรุงเทพฯ จะเป็นเมืองในฝันของเหล่า Expat แทบทุกกลุ่ม แต่ผู้ที่ทำหน้าที่เป็นตัวแทนขายอสังหาริมทรัพย์ ที่มี Certificate ระดับสูงสุดในเมืองไทยยังมีอยู่น้อยมาก คุณณัฐชานันท์ จึงอยากยกระดับมาตรฐานตัวแทนขายอสังหาริมทรัพย์ของไทยให้ดีกว่าที่เป็นอยู่ ด้วยการเปิดคอร์สสอนความรู้เกี่ยวกับอสังหาริมทรัพย์ ตั้งแต่เริ่มต้น จนถึงทำการเช่า การขาย การทำเงินจากอสังหาฯ มีอะไรบ้าง การให้เช่าหรือขายสินทรัพย์ด้วยตนเอง กฎหมายที่ควรรู้ รวมถึงข้อมูลพิเศษเรื่องการลงทุนอสังหาริมทรัพย์ในต่างแดน อย่าง ประเทศอังกฤษ โดยเนื้อหาทั้งหมดมีเอกสารประกอบ พร้อมคู่มือที่จะนำไปใช้ได้ง่ายๆ ประหยัดเวลา และประหยัดค่าใช้จ่าย ในช่วงเดือนพฤษภาคม 2563 นี้ ติดตามรายละเอียดได้ที่ www.facebook.com/thepropsthailandคุณณัฐชานันท์ ทิ้งท้ายว่างานนี้จะจัดเต็มเรื่องความรู้จากประสบการณ์ที่มี ไม่ใช่แค่อยากให้เมืองไทยเป็นที่อยู่ในฝันของเหล่า Expat เอลิสต์ทั่วโลกต่อไปเรื่อย ๆ เท่านั้น แต่ยังอยากที่จะยกระดับ Real Estate Agent ของไทยให้ได้มาตรฐานแบบเดียวกับระดับสากล และเป็นมากกว่านายหน้าซื้อขายอย่างที่ลูกค้าเข้าใจตอนนี้อยากบอกว่าน้ำหนักกูทะลุเพดานสูงกว่ากราฟตลาดหุ้นมากตอนนี้หลังจากที่ต้องทำงานที่บ้านเดินไกลสุดหน้าบ้านหลังบ้าน20ก้าวคือทั่วบ้านแล้วแม่กูก็ดีจังลูกหลานต้องอยู่บ้านเพราะไปไหนก็ไม่น่าปลอดภัยทำโน่นนี่นั้นให้กิน ไม่กินก็งอนกูผิดอีก ผลคือตอนนี้กูจะกลิ้งได้แล้วข้าวอะไม่เท่าไหร่ร้อนๆมานี่เปิดตู้เย็นตลอดนมเปรี้ยว นมชมพู ยาคูลย์ ไอติม ตุนไว้สักเดือนหมดภายในสามวัน กูไม่ตายเพราะโควิดอะ จะตายเพราะเบาหวานแทนโล๊ะให้หมดจ้า​ ขอคนพร้อมโอนน่าจ้าจัดส่งพรุ่งนี้คะ​ พิจารณาก่อนจองน่า​ค่าส่งลดเหลือ25จ้า​มีของแถมให้ทุกบ้านน่าทีมแม่ๆต้นเดือนพฤษภา มีใครเจ็บเตือนแล้วบ้างไหมค่ะ บ้านนี้เจ็บปวดเตือนมา2-3วันแล้วจ้าาา จะถึงกำหนดไหม😂🥰👧🏻👶🏻บ้านนี้ท้อง2จ้าสอบถามแม่ๆค่ะ ปวดท้องเหมือนกับเปนเมนร้าวไปหลัง แต่ท้องไม่แข็ง ไม่มีมูก ปกติมั้ยค่ะ34+1wkสอบถามแม่ๆที่มีมูกใส ไปโรงบาลแล้วโดนหมอตรวจปากมดลูกแต่ปากมดลูกยังไม่เปิด กลับมามีอาการเจ็บท้องมั้ยค่ะ บ้านนี้ปวดทุก5นาทีเลยแต่เค้าให้กลับบ้าน กลัวกลับไปเค้าให้กลับบ้านอีกขอบอกว่า อร่อยจิงครับแนะนำเลย แต่อยากให้ลดเค็มหน่อยครับ ฝากเบทาโกด้วย ผมซื้อตุนไว้4-5แพคตลอดสวัสดีค่ะ ขออนุญาตแอดมินเปิดรับออเดอร์ไส้กรอกอีสานกับทอดมันข้าวโพดนะคะ สนใจสั่งได้สินค้าจัดส่งวันจันทร์ที่30คะเกือบทั้งหมดซื้อจากห้าง/ช็อปปี้ เป็นของมือสองของเรานะคะ ไม่ใช่มือสามมือสี่ สภาพดีแน่นอน ซักทำความสะอาดให้หมดแล้ว ปักหมุดไว้เลยค่ะ เดี๋ยวถ่ายรูปลงรายละเอียดอีกทีในคอมเม้นนะคะ📌เปิดรับออเดอร์ปลาสลิดไข่ทอดตัวใหญ่ๆ เท่าฝ่ามือ #ลูกค้าที่พลาดรอบก่อนหน้า รอบนี้จัดเลยจ้า👍 🐟🐟ปลาสลิดไข่ทอดพร้อมทาน🐟🐟เมื่อ jeban x tom ford beauty ส่ง TOM FORD SHADE AND ILLUMINATE SOFT RADIANCE FOUNDATION SPF 50/PA++++ มาให้ วันนี้ตั้งใจจะแต่งหน้าด้วย TOM Ford ทั้งหมด งานแป้ง งานลิป งานตา งานคิ้ว เลยมานั่งรื้อ TOM FORDออกมาบางส่วน เพิ่งรู้ว่าตัวเองเป็นสาวก TOM FORD ก็วันนี้ 😍😍😍😍สีชัดติดทนเกลี่ยมากๆค่ะ ทิ้งไว้นานไม่แห้งด้วย บางแบรนด์ทิ้งไว้ลืมใช้นานๆแห้งจนทาไม่ได้เลยค่ะเคยใช้ set ทดลองค่ะ ดีงามค่ะ ส่วนตัวผิวแห้ง ชุ่มชื่นดี แต่วิธีใช้ เยอะแยะสิ่งไปมาก เรียงกัน 8 ตัว เลย พักก๊อนนน ขี้เกียดจงข้ามไปยินดีแบ่งปันสาวๆ บ้านจีบันทุกคนค่าขอคำแนะนำหน่อยคะ​คือเราเป็นสิวเยอะคะ​เป็รอยดำด้วย​ ต้องเริ่มใช้แบบใหนก่อนตัวใหนก่อนคะ​ ดู​ จขกท แล้ว​ มีสิวเหมือนเราเลยล้างมือแทบตาย สุดท้ายเชื้อโรคเต็มกระเป๋าสตางค์ แบบนี้ไม่โอเคน้าาาาาน่ากลัวจังค่ะ มีอาการต้องรีบไปหาหมอเลยนะค่ะ เชื่อว่า คนที่อาการเเบบคุณลุง ต้องมีอีกแน่นอน เสียใจกับครอบครัวด้วยนะค่ะเผื่อสนใจปี15รถผมขายเองมือแรกป้ายแดงไมล์6หมื่นแท้เข้าศูนย์ทุกระยะครับไฟแนนไม่พักชำระหนี้ไปร้องเรียนใด้ที่ไหนค่ะใครรุ้มั่งขอบคุณค้าบบบบ ฝากกดสับตะไคร้ แชแนลยูทูปของเราด้วยน้าา😂ดูหน้าตาไม่ค่อยเต็มใจนะคะ เปลี่ยนอาชีพดีกว่าไหม?🤣ขออนุญาตฝากร้านค่ะ แป้งตลับเอนวี่ตลับดำเรามีพร้อมส่งจ้า สนใจทักมาได้จ้าLine:tuy58ได้น้องมาใหม่ ช่วยตั้งชื่อหน่อยค่ะ (ด.ช.) ขอ 2 พยางค์ ขึ้นต้นสระ-โ ลงท้ายสระ -าโดนกันเป็นทอดๆค่ะ แอดก็ปฎิเสธงานช่วงนั้นหมดทุกงานเลยเหมือนกัน><! ก็เสียโอกาส แต่ช่างมันค่ะ เอาเป็นว่าถือว่าดวงเราจะต้องเป็นแบบนี้ละกันค่ะที่ออสเตรเลียตอนนี้บางสายการบินให้พนักงานมาทำงานเติมของที่ supermarket แล้วค่ะน่ารักหน้าเอ็นดู หน้ากากเด็กมันหายากอะเนอะรูกกกกกกกพวกมะนุดหายหัวไปไหนหมด เรายึดสนามบินไว้หมดแล้ว โลกต้องเป็นของเรา 😸ชอบมากค่ะสวยค่ะอยากยุ่แบบนี้นะค่ะขอบคุนเจ้างพาภมากค่ะคนรวยมันหวงเงินจะตายไป กลัวตัวเลขการจัดลำดับมหาเศษผีจะตกลง ปั่นเงินกันจนตายห่าคาธนบัตรนั่นแหละ มันหวังขนเอาไปใช้ในปรโลกกันทั้งนั้นโรคนี้อาจจะเคยเกิดขึ้นเมื่อหมื่นปีมาแล้วก็อาจเป็นได้บางศพถูกเอาไปเผาแต่บางศพถูกฝั่งเอาไว้แล้วคนยุคนี้มือบอนไปขุดเอาศพที่ฝังเอาไว้ขึ้นมาพร้อมกับเชื้อโรคตัวนี้ต่อนี้ศพที่เผาเชื้อก็จะตายแล้วศพที่ฝังถ้าต่อไปอีกร้อยปีมีคนไปขุดขึ้นมาแล้วเจอถุงคงจะต้องอยากรู้อย่ากเห็นว่ามันเป็นอะไรเชื้อโรคจะกลายสายพันก็จะระบาดขึ้นมาอีกรฟม. ขอความร่วมมือผู้โดยสารรถไฟฟ้า MRT กรอกแบบคำถามสุขภาพ (ต.8 คค ) เมื่อเดินทางข้ามเขตพื้นที่จังหวัดเรื่องนี้เราวนเป็น10รอบ หลงมาหลายปียังหาทางออกไม่ได้เลยยยค่ะ ในยูทูปก็มี ว่าแล้วก็ไปวนอีกซักรอบดีกว่า555555ตอนนี้กำลังเริ่มดูเลขาคิม มาสองตอนแรกดีมากกกไปสอยมามะวาน ราคานี้จริงๆๆ เสียดายมีเเต่32g รายละเอียดเท่าที่ทราบอยู่ในคอมเม้นต์ค่ะ ตอบไปส่วนนึงเเล้ว ไม่ต้องทัก มาถามส่วนตัวนะคะชอบคำเคลมตรงเงินเดือนมากค่ะ ถ้าทำงานได้ตามที่พูด เท่าไหร่ก็จ่ายได้หมด😆👏👏พายเปี๊ยะกรอบ ไส้หมูหยองพริกเผาค่ะ กล่อง100 บาท7ชิ้น ใช้เนยสดหอมอร่อยๆอีเหี้ยเอ๊ยยยยยย ตอนแรกก็ว่าหล่อแล้วนะ ปลื้มแล้วนะ มาเจอชอทเล่นกีตาร์คือออออออบวกไปเลยแม่พันพันล้านคะแนน เอาไปเลยทั้งใจ ทั้งตัว ทั้งสมุดบัญ เอาไปเล้ยยยยยยยยยรอบหน้าเชิญคุณทิมมาหน่อยนะแม่มีใครอ่านเรื่องนี้บ้าง สนุกมากกกกกกกก จนต้องซื้อเก็บน้องก็ไม่กล้าลงแข่งพี่ท็อฟ เพราะเรายังไม่เก่งเหมือนกัน และ Toxic Commentข้าวผัดเนยกระเทียมกับเครื่องปรุงหอมๆ ทานคู่กับเนื้อสไลด์ราดซอสฉ่ำๆอร่อยเหาะในราคามิตรภาพ ตุ๊กแกรับทำข้าวกล่องด้วยนะฮะสั่งเยอะลดจ้าาาาาอุปกรณ์เครื่องออกกำลังกายที่สวนสาธารณะ ราวบันได ราวสะพาน ด้วยซิคับ..ขออนุญาตินำภาพไปเป็นปก fb ได้มั้ยคะ ชอบภาพสวยมากเลย ชอบดูทุ่งบัวค่ะเสียดายจัง มะนาวไม่มีน้ำรด ลูกหล่นหมดเลย ตอนนี้โลเป็นร้อย ถ้าไม่หล่นคงได้หลายตังค์ ฝนเพิ่งตกเมื่อวานครับออกมาพ่นที่ตลาดสินทองหน้ารพ.พระนั่งเกล้าบ้างซิ🥺🥺🥺🥺แฟนคลับหงส์แดงชาวไทย ฝากข้อความไปถึงแฟนคลับหงส์แดงทั่วโลกครัฟดินหมักยังไม่ดีพอค่ะยังเป็นแห้งแกรนๆ หาความร่วนซุยไม่ได้ต้นกล้าเลยเหลืองแกรนๆค่ะก่อนส่งเขาฟองละ4.ขายยากด้วยตอนนี้ส่ง5.บ.ไม่พอส่งแปลกเหมือนกันถ้าเราเป็นรัฐบาล ก้อเหนื่อยนะ เชื้อโรคเอง ไฟป่าเอง ไฟป่านี่ไม่รู้คนเผาหาของป่าอะป่าวอันนี้ถามจริงจัง หาแอลกอฮอล์ยากเย็น ใช้เหล้าขาว ว๊อดก้า ยิน อะไรทำนองนี้ พอปะทัง ได้มั้ย มันสามารถฆ่าเชื้อได้บ้างมั้ยคะ ไม่ได้กวนจริงๆนะคะอิคนนี้เลยค่ะจ้องจะขายครอสอาหารเสริมมังสาวิรัติแพงๆล่าสุดคนแชร์คลิป โควิดไม่น่ากลัวไปเป็นหมื่นแชร์ นางบอกกินข้าวโอ๊ตกินผัก อย่ากินเนื้อร่างกายจะได้แข๊งแรง \n",
      "10243\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(1, 'เดอะ'), (2, ' '), (3, 'พร็อพส์'), (4, ' '), (5, 'กว่า'), (6, ' '), (7, '15'), (8, ' '), (9, 'ปี'), (10, ' '), (11, 'บน'), (12, 'เส้นทาง'), (13, 'ตัวแทน'), (14, 'เช่า'), (15, '-'), (16, 'ซื้อ'), (17, 'อสังหาริมทรัพย์'), (18, ' '), (19, 'สร้าง'), (20, 'เมือง'), (21, 'ไทย'), (22, 'เป็น'), (23, 'ที่อยู่'), (24, 'ใน'), (25, 'ฝัน'), (26, 'ของ'), (27, ' '), (28, 'Expat'), (29, ' '), (30, 'เอ'), (31, 'ลิสต์'), (32, 'ทั่วโลก'), (33, 'การ'), (34, 'ติดอันดับ'), (35, 'เมือง'), (36, 'น่าอยู่'), (37, 'ของ'), (38, ' '), (39, '“'), (40, 'กรุงเทพฯ'), (41, '”'), (42, ' '), (43, 'จาก'), (44, 'นิตยสาร'), (45, 'หรือ'), (46, 'องค์กร'), (47, 'การ'), (48, 'จัดอันดับ'), (49, 'ระดับนานาชาติ'), (50, 'หลาย'), (51, 'แห่ง'), (52, 'นั้น'), (53, ' '), (54, 'ไม่'), (55, 'ใช่'), (56, 'ตำแหน่ง'), (57, 'ที่'), (58, 'หา'), (59, 'ซื้อ'), (60, 'มา'), (61, 'ได้'), (62, 'ด้วย'), (63, 'เงินทอง'), (64, 'หรือ'), (65, 'ใช้'), (66, 'อิทธิพล'), (67, 'ใด'), (68, ' '), (69, 'ๆ'), (70, ' '), (71, 'หาก'), (72, 'แต่'), (73, 'เป็น'), (74, 'ความจริง'), (75, 'ที่'), (76, 'ชาวต่างชาติ'), (77, 'ผู้'), (78, 'เคย'), (79, 'มา'), (80, 'เยือน'), (81, ' '), (82, 'หรือ'), (83, 'พักอาศัย'), (84, 'ใช้ชีวิต'), (85, 'อยู่'), (86, 'ซึ่ง'), (87, 'มัก'), (88, 'ถูก'), (89, 'เรียก'), (90, 'สั้น'), (91, ' '), (92, 'ๆ'), (93, ' '), (94, 'ว่า'), (95, ' '), (96, '‘'), (97, 'Expat'), (98, '’'), (99, ' '), (100, 'ที่'), (101, 'มาจาก'), (102, 'คำเต็ม'), (103, 'ว่า'), (104, ' '), (105, '“'), (106, 'Expatriate'), (107, '”'), (108, ' '), (109, 'ต่าง'), (110, 'ยอมรับ'), (111, ' '), (112, 'และ'), (113, 'ยังคง'), (114, 'มองว่า'), (115, 'เป็น'), (116, 'เมือง'), (117, 'น่าอยู่'), (118, 'เสมอ'), (119, 'ไม่'), (120, 'ใช่'), (121, 'เพียง'), (122, 'เพราะ'), (123, 'ความพร้อม'), (124, 'ของ'), (125, 'ที่พัก'), (126, 'อาศัยที่'), (127, 'มี'), (128, 'ให้'), (129, 'เลือก'), (130, 'หลาก'), (131, 'รูปแบบ'), (132, 'ตาม'), (133, 'ความต้องการ'), (134, 'และ'), (135, 'งบประมาณ'), (136, 'ที่'), (137, 'ได้รับ'), (138, ' '), (139, 'หรือ'), (140, 'สิ่งอำนวยความสะดวก'), (141, 'ต่าง ๆ'), (142, ' '), (143, 'ที่'), (144, 'กรุงเทพฯ'), (145, ' '), (146, 'พร้อม'), (147, 'จะ'), (148, 'ตอบ'), (149, 'โจทย์'), (150, 'ให้'), (151, 'กับ'), (152, 'กลุ่ม'), (153, ' '), (154, 'Expat'), (155, '\\xa0'), (156, 'ได้'), (157, 'มากกว่า'), (158, 'ประเทศ'), (159, 'อื่น'), (160, ' '), (161, 'ๆ'), (162, ' '), (163, 'ใน'), (164, 'แถบ'), (165, 'เอเชีย'), (166, ' '), (167, 'แต่'), (168, 'เบื้องหลัง'), (169, 'สิ่ง'), (170, 'ที่'), (171, 'ทำให้'), (172, 'ชาวต่างชาติ'), (173, 'กลุ่ม'), (174, 'นี้'), (175, 'มั่นใจ'), (176, 'และ'), (177, 'เชื่อมั่น'), (178, 'ก็'), (179, 'คือ'), (180, ' '), (181, 'ตัวแทน'), (182, 'เช่า'), (183, '-'), (184, 'ซื้อ'), (185, 'อสังหาริมทรัพย์'), (186, 'ระดับ'), (187, 'อินเตอร์เนชั่นแนล'), (188, 'อย่าง'), (189, '\\xa0'), (190, 'คุณ'), (191, 'ณัฐ'), (192, 'ชา'), (193, 'นันท์'), (194, ' '), (195, 'จันทรา'), (196, 'นุ'), (197, 'วัฒน'), (198, '์'), (199, ' '), (200, 'ผู้ก่อตั้ง'), (201, 'และ'), (202, 'ซีอีโอ'), (203, 'ของ'), (204, ' '), (205, 'บริษัท'), (206, ' '), (207, 'เดอะ'), (208, ' '), (209, 'พร็อพส์'), (210, ' '), (211, 'จำกัด'), (212, '\\xa0'), (213, 'หนึ่ง'), (214, 'ใน'), (215, 'ผู้'), (216, 'ที่'), (217, 'ทำให้'), (218, 'กรุงเทพฯ'), (219, ' '), (220, 'เป็น'), (221, 'เมือง'), (222, 'ใน'), (223, 'ฝัน'), (224, 'ของ'), (225, 'เหล่า'), (226, ' '), (227, '‘'), (228, 'Expat'), (229, '’'), (230, ' '), (231, 'ระดับ'), (232, 'เอ'), (233, 'ลิสต์'), (234, ' '), (235, 'มา'), (236, 'ตลอด'), (237, 'ระยะเวลา'), (238, 'เกือบ'), (239, ' '), (240, '15'), (241, ' '), (242, 'ปี'), (243, ' '), (244, 'ที่ผ่านมา'), (245, '“'), (246, 'ย้อน'), (247, 'ไป'), (248, ' '), (249, '14'), (250, ' '), (251, '-'), (252, ' '), (253, '15'), (254, ' '), (255, 'ปีก่อน'), (256, ' '), (257, 'Real'), (258, ' '), (259, 'Estate'), (260, ' '), (261, 'Agent'), (262, ' '), (263, 'ยัง'), (264, 'เป็นเรื่อง'), (265, 'ใหม่'), (266, 'มาก'), (267, 'สำหรับ'), (268, 'คนไทย'), (269, ' '), (270, 'เรา'), (271, 'เอง'), (272, 'เริ่ม'), (273, 'รู้จัก'), (274, 'อาชีพ'), (275, 'ตัวแทน'), (276, 'ขาย'), (277, 'อสังหาริมทรัพย์'), (278, 'จาก'), (279, 'รายการโทรทัศน์'), (280, ' '), (281, 'ตอน'), (282, 'เรียน'), (283, 'อยู่'), (284, 'ที่'), (285, 'อเมริกา'), (286, ' '), (287, 'แล้ว'), (288, 'รู้สึก'), (289, 'ว่า'), (290, 'ชอบ'), (291, 'มาก'), (292, ' '), (293, 'พอ'), (294, 'เรียนจบ'), (295, 'กลับมา'), (296, 'ที่'), (297, 'เมือง'), (298, 'ไทย'), (299, ' '), (300, 'ก็'), (301, 'ไป'), (302, 'สมัครงาน'), (303, 'ด้าน'), (304, 'นี้'), (305, 'กับ'), (306, 'บริษัท'), (307, 'ที่'), (308, 'มี'), (309, 'อยู่'), (310, 'ใน'), (311, 'ช่วง'), (312, 'นั้น'), (313, ' '), (314, 'เจ้าของ'), (315, 'เป็น'), (316, 'ชาว'), (317, 'อังกฤษ'), (318, 'ที่'), (319, 'มาจาก'), (320, 'องค์กร'), (321, 'ระดับ'), (322, 'อินเตอร์เนชั่นแนล'), (323, 'ขนาดใหญ่'), (324, ' '), (325, 'มี'), (326, 'ความเข้าใจ'), (327, 'ความ'), (328, 'เป็น'), (329, 'เอเชีย'), (330, ' '), (331, 'และ'), (332, 'มี'), (333, 'รูปแบบ'), (334, 'การสอน'), (335, 'งาน'), (336, 'แบบ'), (337, 'ตัวต่อตัว'), (338, ' '), (339, 'ทำให้'), (340, 'เรา'), (341, 'สามารถ'), (342, 'เรียน'), (343, 'รู้งาน'), (344, 'ได้'), (345, 'อย่าง'), (346, 'รวดเร็ว'), (347, ' '), (348, 'เริ่ม'), (349, 'จาก'), (350, 'ทำ'), (351, 'เรื่อง'), (352, 'เช่า'), (353, 'กับ'), (354, 'ลูกค้า'), (355, 'ต่าง'), (356, 'ชาติก่อน'), (357, ' '), (358, 'ซึ่ง'), (359, 'เรา'), (360, 'ชอบ'), (361, 'อยู่แล้ว'), (362, ' '), (363, 'และ'), (364, 'มี'), (365, 'ความคุ้นเคย'), (366, 'กับ'), (367, 'วัฒนธรรม'), (368, 'ของ'), (369, 'พวกเขา'), (370, ' '), (371, 'ทำให้'), (372, 'เรา'), (373, 'รู้'), (374, 'ว่า'), (375, ' '), (376, 'Expat'), (377, ' '), (378, 'ที่'), (379, 'เข้ามา'), (380, 'อยู่'), (381, 'ที่'), (382, 'เมือง'), (383, 'ไทย'), (384, 'เขา'), (385, 'มี'), (386, 'ความชอบ'), (387, 'แตก'), (388, 'ต่างกัน'), (389, 'ยังไง'), (390, ' '), (391, 'หลังจากนั้น'), (392, 'ก็'), (393, 'ออกมา'), (394, 'เปิด'), (395, 'บริษัท'), (396, 'ของ'), (397, 'ตัวเอง'), (398, 'ค่ะ'), (399, '”'), (400, 'คุณ'), (401, 'ณัฐ'), (402, 'ชา'), (403, 'นันท์'), (404, ' '), (405, 'เผย'), (406, 'จุดเริ่มต้น'), (407, 'ก่อน'), (408, 'มา'), (409, 'เปิด'), (410, 'บริษัท'), (411, ' '), (412, 'เดอะ'), (413, ' '), (414, 'พร็อพส์'), (415, ' '), (416, 'เป็น'), (417, 'ของ'), (418, 'ตัวเอง'), (419, ' '), (420, 'จากนั้น'), (421, 'บริษัท'), (422, 'ก็ได้'), (423, 'สะสม'), (424, 'ชื่อเสียง'), (425, ' '), (426, 'ทำหน้าที่'), (427, ' '), (428, 'Service'), (429, ' '), (430, '&'), (431, ' '), (432, 'Solution'), (433, ' '), (434, 'ด้าน'), (435, 'อสังหาริมทรัพย์'), (436, ' '), (437, 'อย่าง'), (438, 'ครบวงจร'), (439, ' '), (440, 'ไม่'), (441, 'ว่า'), (442, 'จะ'), (443, 'เช่า'), (444, ' '), (445, 'ซื้อ'), (446, ' '), (447, 'หรือ'), (448, 'แม้'), (449, 'เรื่อง'), (450, 'การลงทุน'), (451, ' '), (452, 'จน'), (453, 'เป็นที่'), (454, 'ไว้เนื้อเชื่อใจ'), (455, 'ของ'), (456, 'เหล่า'), (457, ' '), (458, 'Expat'), (459, ' '), (460, 'โดยเฉพาะ'), (461, 'กลุ่ม'), (462, 'เอ'), (463, 'ลิสต์'), (464, ' '), (465, 'อย่าง'), (466, 'เอกอัครราชทูต'), (467, 'จาก'), (468, 'ประเทศ'), (469, 'ต่าง ๆ'), (470, ' '), (471, 'ซึ่ง'), (472, 'เธอ'), (473, 'เล่า'), (474, 'ต่อ'), (475, 'ถึง'), (476, 'แนว'), (477, 'ทางใน'), (478, 'การ'), (479, 'ดำเนิน'), (480, 'ธุรกิจ'), (481, ' '), (482, 'จน'), (483, 'ประสบความสำเร็จ'), (484, 'อย่างเช่น'), (485, 'ทุกวันนี้'), (486, 'ว่า'), (487, '“'), (488, 'สิ่ง'), (489, 'ที่'), (490, 'ทำให้'), (491, 'เรา'), (492, 'ได้รับ'), (493, 'ความไว้วางใจ'), (494, 'จาก'), (495, ' '), (496, 'Expat'), (497, ' '), (498, 'กลุ่ม'), (499, 'นี้'), (500, 'มาโดยตลอด'), (501, ' '), (502, 'คือ'), (503, ' '), (504, 'การ'), (505, 'บริการ'), (506, 'ที่'), (507, 'เรา'), (508, 'โฟกัส'), (509, 'มาก'), (510, ' '), (511, 'ๆ'), (512, ' '), (513, 'เพราะ'), (514, 'เรา'), (515, 'รัก'), (516, 'ใน'), (517, 'อาชีพ'), (518, 'นี้'), (519, 'ใน'), (520, 'ฐานะ'), (521, 'ผู้ก่อตั้ง'), (522, 'บริษัท'), (523, ' '), (524, 'จึง'), (525, 'มี'), (526, 'ความตั้งใจ'), (527, 'ใน'), (528, 'การ'), (529, 'ให้บริการ'), (530, 'กับ'), (531, 'ลูกค้า'), (532, 'อย่าง'), (533, 'ดี'), (534, 'มา'), (535, 'ตั้งแต่'), (536, 'วินาที'), (537, 'แรก'), (538, 'ที่ทำงาน'), (539, ' '), (540, 'สมมติ'), (541, 'ว่า'), (542, 'ลูกค้า'), (543, 'ต้อง'), (544, 'การเช่า'), (545, 'อสังหาริมทรัพย์'), (546, 'สัก'), (547, 'แห่ง'), (548, ' '), (549, 'เรา'), (550, 'ต้อง'), (551, 'จัดเตรียม'), (552, 'ข้อมูล'), (553, 'ทุกอย่าง'), (554, ' '), (555, 'ดูแล'), (556, 'ลูกค้า'), (557, 'ตั้งแต่'), (558, 'เข้า'), (559, 'มาถึง'), (560, 'เมือง'), (561, 'ไทย'), (562, ' '), (563, 'จนถึง'), (564, 'การพา'), (565, 'ไป'), (566, 'เลือก'), (567, 'ที่พัก'), (568, 'ระยะยาว'), (569, ' '), (570, 'การ'), (571, 'แนะนำ'), (572, 'โรงเรียน'), (573, 'ให้'), (574, 'กับ'), (575, 'ลูก'), (576, ' '), (577, 'ๆ'), (578, ' '), (579, 'ในกรณีที่'), (580, 'เขา'), (581, 'พา'), (582, 'ครอบครัว'), (583, 'มา'), (584, 'ด้วย'), (585, ' '), (586, 'การ'), (587, 'เตรียม'), (588, 'ข้อมูล'), (589, 'ถึง'), (590, 'สถานที่'), (591, 'อำนวยความสะดวก'), (592, 'บริเวณ'), (593, 'โดยรอบ'), (594, 'ที่พักอาศัย'), (595, ' '), (596, 'ทั้ง'), (597, 'แหล่ง'), (598, 'ช็อปปิ้ง'), (599, ' '), (600, 'สถานที่'), (601, 'ทำกิจกรรม'), (602, 'ของ'), (603, 'ครอบครัว'), (604, 'ตาม'), (605, 'ไลฟ์สไตล์'), (606, ' '), (607, 'เรื่อง'), (608, 'การท่องเที่ยว'), (609, ' '), (610, 'ตลอดจน'), (611, 'ช่วย'), (612, 'หา'), (613, 'แม่บ้าน'), (614, ' '), (615, 'และ'), (616, 'คนขับรถ'), (617, ' '), (618, 'อย่าง'), (619, 'น้อย'), (620, 'เรา'), (621, 'ต้อง'), (622, 'ส่ง'), (623, 'ลิสต์'), (624, 'ให้'), (625, 'เขา'), (626, 'รับรู้'), (627, 'ก่อน'), (628, 'ไม่'), (629, 'น้อยกว่า'), (630, ' '), (631, '3'), (632, ' '), (633, 'แห่ง'), (634, 'ขั้นต่ำ'), (635, ' '), (636, 'ตาม'), (637, 'ความต้องการ'), (638, ' '), (639, 'และ'), (640, 'ทีมงาน'), (641, 'ทุกคน'), (642, 'สามารถ'), (643, 'เห็น'), (644, 'ความต้องการ'), (645, 'ของ'), (646, 'ลูกค้า'), (647, 'ทุกราย'), (648, ' '), (649, 'ผ่าน'), (650, 'ดา'), (651, 'ต้า'), (652, 'ขนาดใหญ่'), (653, 'ส่วนกลาง'), (654, 'ของ'), (655, 'บริษัท'), (656, ' '), (657, 'ทำให้'), (658, 'ลูกค้า'), (659, 'สามารถ'), (660, 'ได้รับ'), (661, 'การ'), (662, 'ดูแลเอาใจใส่'), (663, 'อย่าง'), (664, 'ทันท่วงที'), (665, 'แม้'), (666, 'ใน'), (667, 'กรณี'), (668, 'เกิด'), (669, 'ปัญหา'), (670, 'ติดขัด'), (671, 'ต่างๆ'), (672, '”'), (673, 'ไม่'), (674, 'ใช่'), (675, 'เพียงแค่'), (676, 'การ'), (677, 'เสาะแสวงหา'), (678, 'คอนโดมิเนียม'), (679, ' '), (680, 'หรือ'), (681, 'อพาร์ตเมนต์'), (682, 'ที่'), (683, 'ตอบ'), (684, 'โจทย์'), (685, 'กับ'), (686, 'กลุ่ม'), (687, 'ลูกค้า'), (688, 'หลัก'), (689, 'เท่านั้น'), (690, ' '), (691, 'แต่'), (692, ' '), (693, 'เดอะ'), (694, ' '), (695, 'พร็อพส์'), (696, ' '), (697, 'ภายใต้'), (698, 'การนำ'), (699, 'ของ'), (700, 'คุณ'), (701, 'ณัฐ'), (702, 'ชา'), (703, 'นันท์'), (704, ' '), (705, 'ยัง'), (706, 'ขยาย'), (707, 'ต่อไป'), (708, 'ถึง'), (709, 'การซื้อขาย'), (710, 'สำหรับ'), (711, 'ชาวต่างชาติ'), (712, 'ที่'), (713, 'ต้องการ'), (714, 'มากกว่า'), (715, 'แค่'), (716, 'การ'), (717, 'มา'), (718, 'พำนัก'), (719, 'ใน'), (720, 'ห้วงเวลา'), (721, 'หนึ่ง'), (722, '“'), (723, 'ลูกค้า'), (724, 'บางคน'), (725, 'มา'), (726, 'อยู่'), (727, 'แล้วก็'), (728, 'มองข้าม'), (729, 'ไป'), (730, 'ถึง'), (731, 'เรื่อง'), (732, 'การลงทุน'), (733, 'ด้วย'), (734, ' '), (735, 'ซึ่ง'), (736, ' '), (737, 'เดอะ'), (738, ' '), (739, 'พร็อพส์'), (740, ' '), (741, 'ก็'), (742, 'พร้อม'), (743, 'บริการ'), (744, 'อย่าง'), (745, 'มืออาชีพ'), (746, 'ใน'), (747, 'มาตรฐานสากล'), (748, ' '), (749, 'สำหรับ'), (750, 'กรณี'), (751, 'ซื้อ'), (752, 'ลงทุน'), (753, 'หลังจาก'), (754, 'ลูกค้า'), (755, 'ตกลง'), (756, 'ซื้อ'), (757, 'เป็นที่'), (758, 'เรียบร้อย'), (759, 'แล้ว'), (760, ' '), (761, 'เรา'), (762, 'จะ'), (763, 'ทำหน้าที่'), (764, 'บริหาร'), (765, 'จัดการ'), (766, 'ให้'), (767, 'หมด'), (768, ' '), (769, 'ตั้งแต่'), (770, ' '), (771, 'หา'), (772, 'ผู้เช่า'), (773, ' '), (774, 'เก็บ'), (775, 'ค่าเช่า'), (776, 'ส่ง'), (777, 'ไป'), (778, 'ให้'), (779, 'ยัง'), (780, 'ต่างประเทศ'), (781, ' '), (782, 'จัด'), (783, 'การซ่อมแซม'), (784, 'ห้อง'), (785, 'หลัง'), (786, 'ผู้เช่า'), (787, 'ออก'), (788, ' '), (789, 'รับ'), (790, 'คำร้องเรียน'), (791, 'ต่าง ๆ'), (792, ' '), (793, 'เสมือน'), (794, 'เรา'), (795, 'เป็น'), (796, 'แลนด์'), (797, 'ลอร์ด'), (798, 'ตัวเอง'), (799, ' '), (800, 'อย่าง'), (801, 'ลูกค้า'), (802, 'บางราย'), (803, ' '), (804, 'ซื้อ'), (805, 'อสังหาริมทรัพย์'), (806, 'ต่าง ๆ'), (807, ' '), (808, 'เอาไว้'), (809, 'แล้ว'), (810, ' '), (811, 'ก็'), (812, 'ไม่'), (813, 'กลับมา'), (814, 'เมือง'), (815, 'ไทย'), (816, 'อีก'), (817, 'นาน'), (818, 'หลาย'), (819, 'ปี'), (820, ' '), (821, 'ปล่อย'), (822, 'ให้'), (823, 'เรา'), (824, 'ดำเนินการ'), (825, 'ทุกอย่าง'), (826, 'เลย'), (827, ' '), (828, 'หรือ'), (829, 'บางราย'), (830, 'ให้'), (831, 'เรา'), (832, 'ทำ'), (833, 'การตลาด'), (834, 'แล้ว'), (835, 'ขาย'), (836, ' '), (837, 'สิ่ง'), (838, 'เหล่านี้'), (839, 'ล้วน'), (840, 'เกิด'), (841, 'จาก'), (842, 'ความไว้วางใจ'), (843, 'ที่'), (844, 'สะสม'), (845, 'มา'), (846, 'ตลอด'), (847, 'การ'), (848, 'ทำ'), (849, 'ธุรกิจ'), (850, 'ด้าน'), (851, 'นี้'), (852, 'ค่ะ'), (853, '”'), (854, 'นอกเหนือ'), (855, 'งาน'), (856, 'บริการ'), (857, ' '), (858, 'และ'), (859, 'ความ'), (860, 'ไว้เนื้อเชื่อใจ'), (861, 'ตาม'), (862, 'ที่กล่าวมา'), (863, 'แล้ว'), (864, ' '), (865, 'อีก'), (866, 'ส่วนสำคัญ'), (867, ' '), (868, 'นั่น'), (869, 'คือ'), (870, 'เรื่อง'), (871, 'ความ'), (872, 'เป็น'), (873, 'มืออาชีพ'), (874, 'ด้าน'), (875, 'ภาษา'), (876, ' '), (877, 'และ'), (878, 'ความเข้าใจ'), (879, 'ทั้ง'), (880, 'ใน'), (881, 'เชิง'), (882, 'ธุรกิจ'), (883, ' '), (884, 'การลงทุน'), (885, ' '), (886, 'และ'), (887, 'กฎหมาย'), (888, 'เกี่ยวข้อง'), (889, 'กับ'), (890, 'อสังหาริมทรัพย์'), (891, 'ต่าง ๆ'), (892, ' '), (893, 'ใน'), (894, 'เมือง'), (895, 'ไทย'), (896, ' '), (897, 'เดอะ'), (898, ' '), (899, 'พร็อพส์'), (900, ' '), (901, 'จึง'), (902, 'ครองใจ'), (903, 'ไม่เพียงแต่'), (904, ' '), (905, 'Expat'), (906, ' '), (907, 'ระดับ'), (908, 'เอ'), (909, 'ลิสต์'), (910, 'เท่านั้น'), (911, ' '), (912, 'ยัง'), (913, 'ต่อเนื่อง'), (914, 'ไป'), (915, 'ถึง'), (916, 'ดี'), (917, 'เวล'), (918, 'ลอป'), (919, 'เปอร์'), (920, 'ต่าง ๆ'), (921, ' '), (922, 'เจ้าของที่ดิน'), (923, 'และ'), (924, 'นักลงทุน'), (925, 'ใน'), (926, 'เมือง'), (927, 'ไทย'), (928, ' '), (929, 'ซึ่ง'), (930, 'มัก'), (931, 'เชิญ'), (932, 'เธอ'), (933, 'ไป'), (934, 'ให้คำปรึกษา'), (935, ' '), (936, 'ใน'), (937, 'ฐานะ'), (938, 'ผู้เชี่ยวชาญ'), (939, 'ด้าน'), (940, 'อสังหาริมทรัพย์'), (941, 'อย่าง'), (942, 'ครอบคลุม'), (943, 'อีกด้วย'), (944, 'อย่างไรก็ตาม'), (945, 'แม้'), (946, 'กรุงเทพฯ'), (947, ' '), (948, 'จะ'), (949, 'เป็น'), (950, 'เมือง'), (951, 'ใน'), (952, 'ฝัน'), (953, 'ของ'), (954, 'เหล่า'), (955, ' '), (956, 'Expat'), (957, ' '), (958, 'แทบ'), (959, 'ทุก'), (960, 'กลุ่ม'), (961, ' '), (962, 'แต่'), (963, 'ผู้'), (964, 'ที่'), (965, 'ทำหน้าที่'), (966, 'เป็นตัวแทน'), (967, 'ขาย'), (968, 'อสังหาริมทรัพย์'), (969, ' '), (970, 'ที่'), (971, 'มี'), (972, ' '), (973, 'Certificate'), (974, ' '), (975, 'ระดับ'), (976, 'สูงสุด'), (977, 'ใน'), (978, 'เมือง'), (979, 'ไทย'), (980, 'ยังมี'), (981, 'อยู่'), (982, 'น้อย'), (983, 'มาก'), (984, ' '), (985, 'คุณ'), (986, 'ณัฐ'), (987, 'ชา'), (988, 'นันท์'), (989, ' '), (990, 'จึง'), (991, 'อยาก'), (992, 'ยกระดับ'), (993, 'มาตรฐาน'), (994, 'ตัวแทน'), (995, 'ขาย'), (996, 'อสังหาริมทรัพย์'), (997, 'ของ'), (998, 'ไทย'), (999, 'ให้'), (1000, 'ดีกว่า'), (1001, 'ที่'), (1002, 'เป็นอยู่'), (1003, ' '), (1004, 'ด้วย'), (1005, 'การ'), (1006, 'เปิด'), (1007, 'คอร์ส'), (1008, 'สอน'), (1009, 'ความรู้'), (1010, 'เกี่ยวกับ'), (1011, 'อสังหาริมทรัพย์'), (1012, ' '), (1013, 'ตั้งแต่'), (1014, 'เริ่มต้น'), (1015, ' '), (1016, 'จนถึง'), (1017, 'ทำ'), (1018, 'การเช่า'), (1019, ' '), (1020, 'การ'), (1021, 'ขาย'), (1022, ' '), (1023, 'การ'), (1024, 'ทำเงิน'), (1025, 'จาก'), (1026, 'อสังหาฯ'), (1027, ' '), (1028, 'มี'), (1029, 'อะไร'), (1030, 'บ้าง'), (1031, ' '), (1032, 'การ'), (1033, 'ให้เช่า'), (1034, 'หรือ'), (1035, 'ขาย'), (1036, 'สินทรัพย์'), (1037, 'ด้วย'), (1038, 'ตนเอง'), (1039, ' '), (1040, 'กฎหมาย'), (1041, 'ที่'), (1042, 'ควร'), (1043, 'รู้'), (1044, ' '), (1045, 'รวมถึง'), (1046, 'ข้อมูล'), (1047, 'พิเศษ'), (1048, 'เรื่อง'), (1049, 'การลงทุน'), (1050, 'อสังหาริมทรัพย์'), (1051, 'ใน'), (1052, 'ต่างแดน'), (1053, ' '), (1054, 'อย่าง'), (1055, ' '), (1056, 'ประเทศ'), (1057, 'อังกฤษ'), (1058, ' '), (1059, 'โดย'), (1060, 'เนื้อหา'), (1061, 'ทั้งหมด'), (1062, 'มี'), (1063, 'เอก'), (1064, 'สารประกอบ'), (1065, ' '), (1066, 'พร้อม'), (1067, 'คู่มือ'), (1068, 'ที่จะ'), (1069, 'นำไปใช้'), (1070, 'ได้'), (1071, 'ง่ายๆ'), (1072, ' '), (1073, 'ประหยัดเวลา'), (1074, ' '), (1075, 'และ'), (1076, 'ประหยัด'), (1077, 'ค่าใช้จ่าย'), (1078, ' '), (1079, 'ใน'), (1080, 'ช่วง'), (1081, 'เดือน'), (1082, 'พฤษภาคม'), (1083, ' '), (1084, '2563'), (1085, ' '), (1086, 'นี้'), (1087, ' '), (1088, 'ติดตาม'), (1089, 'รายละเอียด'), (1090, 'ได้ที่'), (1091, '\\xa0'), (1092, 'www'), (1093, '.'), (1094, 'facebook'), (1095, '.'), (1096, 'com'), (1097, '/'), (1098, 'thepropsthailand'), (1099, 'คุณ'), (1100, 'ณัฐ'), (1101, 'ชา'), (1102, 'นันท์'), (1103, ' '), (1104, 'ทิ้งท้าย'), (1105, 'ว่า'), (1106, 'งาน'), (1107, 'นี้'), (1108, 'จะ'), (1109, 'จัด'), (1110, 'เต็ม'), (1111, 'เรื่อง'), (1112, 'ความรู้'), (1113, 'จาก'), (1114, 'ประสบการณ์'), (1115, 'ที่'), (1116, 'มี'), (1117, ' '), (1118, 'ไม่'), (1119, 'ใช่'), (1120, 'แค่'), (1121, 'อยาก'), (1122, 'ให้'), (1123, 'เมือง'), (1124, 'ไทย'), (1125, 'เป็น'), (1126, 'ที่อยู่'), (1127, 'ใน'), (1128, 'ฝัน'), (1129, 'ของ'), (1130, 'เหล่า'), (1131, ' '), (1132, 'Expat'), (1133, ' '), (1134, 'เอ'), (1135, 'ลิสต์'), (1136, 'ทั่วโลก'), (1137, 'ต่อไป'), (1138, 'เรื่อย ๆ'), (1139, ' '), (1140, 'เท่านั้น'), (1141, ' '), (1142, 'แต่'), (1143, 'ยัง'), (1144, 'อยาก'), (1145, 'ที่จะ'), (1146, 'ยกระดับ'), (1147, '\\xa0'), (1148, 'Real'), (1149, ' '), (1150, 'Estate'), (1151, ' '), (1152, 'Agent'), (1153, ' '), (1154, 'ของ'), (1155, 'ไทย'), (1156, 'ให้'), (1157, 'ได้มาตรฐาน'), (1158, 'แบบ'), (1159, 'เดียว'), (1160, 'กับ'), (1161, 'ระดับ'), (1162, 'สากล'), (1163, ' '), (1164, 'และ'), (1165, 'เป็น'), (1166, 'มากกว่า'), (1167, 'นายหน้าซื้อขาย'), (1168, 'อย่าง'), (1169, 'ที่'), (1170, 'ลูกค้า'), (1171, 'เข้าใจ'), (1172, 'ตอนนี้'), (1173, 'อยาก'), (1174, 'บอ'), (1175, 'กว่า'), (1176, 'น้ำหนัก'), (1177, 'กู'), (1178, 'ทะลุ'), (1179, 'เพดาน'), (1180, 'สูง'), (1181, 'กว่า'), (1182, 'กราฟ'), (1183, 'ตลาดหุ้น'), (1184, 'มาก'), (1185, 'ตอนนี้'), (1186, 'หลังจากที่'), (1187, 'ต้อง'), (1188, 'ทำงาน'), (1189, 'ที่'), (1190, 'บ้าน'), (1191, 'เดิน'), (1192, 'ไกล'), (1193, 'สุด'), (1194, 'หน้าบ้าน'), (1195, 'หลังบ้าน'), (1196, '20'), (1197, 'ก้าว'), (1198, 'คือ'), (1199, 'ทั่ว'), (1200, 'บ้าน'), (1201, 'แล้ว'), (1202, 'แม่'), (1203, 'กู'), (1204, 'ก็ดี'), (1205, 'จัง'), (1206, 'ลูกหลาน'), (1207, 'ต้อง'), (1208, 'อยู่'), (1209, 'บ้าน'), (1210, 'เพราะ'), (1211, 'ไป'), (1212, 'ไหน'), (1213, 'ก็'), (1214, 'ไม่'), (1215, 'น่า'), (1216, 'ปลอดภัย'), (1217, 'ทำ'), (1218, 'โน่น'), (1219, 'นี่'), (1220, 'นั้น'), (1221, 'ให้'), (1222, 'กิน'), (1223, ' '), (1224, 'ไม่'), (1225, 'กิน'), (1226, 'ก็'), (1227, 'งอน'), (1228, 'กู'), (1229, 'ผิด'), (1230, 'อีก'), (1231, ' '), (1232, 'ผล'), (1233, 'คือ'), (1234, 'ตอนนี้'), (1235, 'กู'), (1236, 'จะ'), (1237, 'กลิ้ง'), (1238, 'ได้'), (1239, 'แล้ว'), (1240, 'ข้าว'), (1241, 'อะ'), (1242, 'ไม่เท่าไหร่'), (1243, 'ร้อน'), (1244, 'ๆ'), (1245, 'มา'), (1246, 'นี่'), (1247, 'เปิด'), (1248, 'ตู้เย็น'), (1249, 'ตลอด'), (1250, 'นมเปรี้ยว'), (1251, ' '), (1252, 'นม'), (1253, 'ชมพู'), (1254, ' '), (1255, 'ยาคู'), (1256, 'ลย์'), (1257, ' '), (1258, 'ไอติม'), (1259, ' '), (1260, 'ตุน'), (1261, 'ไว้'), (1262, 'สัก'), (1263, 'เดือน'), (1264, 'หมด'), (1265, 'ภายใน'), (1266, 'สาม'), (1267, 'วัน'), (1268, ' '), (1269, 'กู'), (1270, 'ไม่'), (1271, 'ตาย'), (1272, 'เพราะ'), (1273, 'โค'), (1274, 'วิด'), (1275, 'อะ'), (1276, ' '), (1277, 'จะ'), (1278, 'ตาย'), (1279, 'เพราะ'), (1280, 'เบาหวาน'), (1281, 'แทน'), (1282, 'โล๊ะ'), (1283, 'ให้'), (1284, 'หมด'), (1285, 'จ้า'), (1286, '\\u200b'), (1287, ' '), (1288, 'ขอ'), (1289, 'คน'), (1290, 'พร้อม'), (1291, 'โอน'), (1292, 'น่า'), (1293, 'จ้า'), (1294, 'จัดส่ง'), (1295, 'พรุ่งนี้'), (1296, 'คะ'), (1297, '\\u200b'), (1298, ' '), (1299, 'พิจารณา'), (1300, 'ก่อน'), (1301, 'จอง'), (1302, 'น่า'), (1303, '\\u200b'), (1304, 'ค่า'), (1305, 'ส่ง'), (1306, 'ลดเหลือ'), (1307, '25'), (1308, 'จ้า'), (1309, '\\u200b'), (1310, 'มี'), (1311, 'ของแถม'), (1312, 'ให้'), (1313, 'ทุก'), (1314, 'บ้าน'), (1315, 'น่า'), (1316, 'ทีม'), (1317, 'แม่'), (1318, 'ๆ'), (1319, 'ต้นเดือน'), (1320, 'พฤษภา'), (1321, ' '), (1322, 'มี'), (1323, 'ใคร'), (1324, 'เจ็บ'), (1325, 'เตือน'), (1326, 'แล้ว'), (1327, 'บ้าง'), (1328, 'ไหม'), (1329, 'ค่ะ'), (1330, ' '), (1331, 'บ้าน'), (1332, 'นี้'), (1333, 'เจ็บปวด'), (1334, 'เตือน'), (1335, 'มา'), (1336, '2'), (1337, '-'), (1338, '3'), (1339, 'วัน'), (1340, 'แล้ว'), (1341, 'จ้า'), (1342, 'าา'), (1343, ' '), (1344, 'จะ'), (1345, 'ถึงกำหนด'), (1346, 'ไหม'), (1347, '😂🥰👧🏻👶🏻'), (1348, 'บ้าน'), (1349, 'นี้'), (1350, 'ท้อง'), (1351, '2'), (1352, 'จ้า'), (1353, 'สอบถาม'), (1354, 'แม่'), (1355, 'ๆ'), (1356, 'ค่ะ'), (1357, ' '), (1358, 'ปวดท้อง'), (1359, 'เหมือนกับ'), (1360, 'เปน'), (1361, 'เมน'), (1362, 'ร้าว'), (1363, 'ไป'), (1364, 'หลัง'), (1365, ' '), (1366, 'แต่'), (1367, 'ท้อง'), (1368, 'ไม่'), (1369, 'แข็ง'), (1370, ' '), (1371, 'ไม่'), (1372, 'มี'), (1373, 'มูก'), (1374, ' '), (1375, 'ปกติ'), (1376, 'มั้ย'), (1377, 'ค่ะ'), (1378, '34'), (1379, '+'), (1380, '1'), (1381, 'wk'), (1382, 'สอบถาม'), (1383, 'แม่'), (1384, 'ๆ'), (1385, 'ที่'), (1386, 'มี'), (1387, 'มูก'), (1388, 'ใส'), (1389, ' '), (1390, 'ไป'), (1391, 'โรงบาล'), (1392, 'แล้ว'), (1393, 'โดน'), (1394, 'หมอ'), (1395, 'ตรวจ'), (1396, 'ปาก'), (1397, 'มดลูก'), (1398, 'แต่'), (1399, 'ปาก'), (1400, 'มดลูก'), (1401, 'ยัง'), (1402, 'ไม่'), (1403, 'เปิด'), (1404, ' '), (1405, 'กลับมา'), (1406, 'มี'), (1407, 'อาการ'), (1408, 'เจ็บท้อง'), (1409, 'มั้ย'), (1410, 'ค่ะ'), (1411, ' '), (1412, 'บ้าน'), (1413, 'นี้'), (1414, 'ปวด'), (1415, 'ทุก'), (1416, '5'), (1417, 'นาที'), (1418, 'เลย'), (1419, 'แต่'), (1420, 'เค้า'), (1421, 'ให้'), (1422, 'กลับบ้าน'), (1423, ' '), (1424, 'กลัว'), (1425, 'กลับ'), (1426, 'ไป'), (1427, 'เค้า'), (1428, 'ให้'), (1429, 'กลับบ้าน'), (1430, 'อีก'), (1431, 'ขอ'), (1432, 'บอ'), (1433, 'กว่า'), (1434, ' '), (1435, 'อร่อย'), (1436, 'จิง'), (1437, 'ครับ'), (1438, 'แนะนำ'), (1439, 'เลย'), (1440, ' '), (1441, 'แต่'), (1442, 'อยาก'), (1443, 'ให้'), (1444, 'ลด'), (1445, 'เค็ม'), (1446, 'หน่อย'), (1447, 'ครับ'), (1448, ' '), (1449, 'ฝาก'), (1450, 'เบ'), (1451, 'ทา'), (1452, 'โก'), (1453, 'ด้วย'), (1454, ' '), (1455, 'ผม'), (1456, 'ซื้อ'), (1457, 'ตุน'), (1458, 'ไว้'), (1459, '4'), (1460, '-'), (1461, '5'), (1462, 'แพ'), (1463, 'ค'), (1464, 'ตลอด'), (1465, 'สวัสดี'), (1466, 'ค่ะ'), (1467, ' '), (1468, 'ขออนุญาต'), (1469, 'แอดมิน'), (1470, 'เปิดรับ'), (1471, 'ออเดอร์'), (1472, 'ไส้กรอก'), (1473, 'อีสาน'), (1474, 'กับ'), (1475, 'ทอดมัน'), (1476, 'ข้าวโพด'), (1477, 'นะคะ'), (1478, ' '), (1479, 'สนใจ'), (1480, 'สั่ง'), (1481, 'ได้'), (1482, 'สินค้า'), (1483, 'จัดส่ง'), (1484, 'วัน'), (1485, 'จันทร์'), (1486, 'ที่'), (1487, '30'), (1488, 'คะ'), (1489, 'เกือบ'), (1490, 'ทั้งหมด'), (1491, 'ซื้อ'), (1492, 'จาก'), (1493, 'ห้าง'), (1494, '/'), (1495, 'ช็อป'), (1496, 'ปี้'), (1497, ' '), (1498, 'เป็น'), (1499, 'ของ'), (1500, 'มือสอง'), (1501, 'ของ'), (1502, 'เรา'), (1503, 'นะคะ'), (1504, ' '), (1505, 'ไม่'), (1506, 'ใช่'), (1507, 'มือสาม'), (1508, 'มือ'), (1509, 'สี่'), (1510, ' '), (1511, 'สภาพ'), (1512, 'ดี'), (1513, 'แน่นอน'), (1514, ' '), (1515, 'ซัก'), (1516, 'ทำความสะอาด'), (1517, 'ให้'), (1518, 'หมด'), (1519, 'แล้ว'), (1520, ' '), (1521, 'ปัก'), (1522, 'หมุด'), (1523, 'ไว้'), (1524, 'เลย'), (1525, 'ค่ะ'), (1526, ' '), (1527, 'เดี๋ยว'), (1528, 'ถ่ายรูป'), (1529, 'ลง'), (1530, 'รายละเอียด'), (1531, 'อีกที'), (1532, 'ใน'), (1533, 'คอม'), (1534, 'เม้น'), (1535, 'นะคะ'), (1536, '📌'), (1537, 'เปิดรับ'), (1538, 'ออเดอร์'), (1539, 'ปลาสลิด'), (1540, 'ไข่'), (1541, 'ทอดตัว'), (1542, 'ใหญ่'), (1543, 'ๆ'), (1544, ' '), (1545, 'เท่า'), (1546, 'ฝ่ามือ'), (1547, ' '), (1548, '#'), (1549, 'ลูกค้า'), (1550, 'ที่'), (1551, 'พลาด'), (1552, 'รอบ'), (1553, 'ก่อนหน้า'), (1554, ' '), (1555, 'รอบ'), (1556, 'นี้'), (1557, 'จัด'), (1558, 'เลย'), (1559, 'จ้า'), (1560, '👍'), (1561, ' '), (1562, '🐟🐟'), (1563, 'ปลาสลิด'), (1564, 'ไข่ทอด'), (1565, 'พร้อม'), (1566, 'ทาน'), (1567, '🐟🐟'), (1568, 'เมื่อ'), (1569, ' '), (1570, 'jeban'), (1571, ' '), (1572, 'x'), (1573, ' '), (1574, 'tom'), (1575, ' '), (1576, 'ford'), (1577, ' '), (1578, 'beauty'), (1579, ' '), (1580, 'ส่ง'), (1581, ' '), (1582, 'TOM'), (1583, ' '), (1584, 'FORD'), (1585, ' '), (1586, 'SHADE'), (1587, ' '), (1588, 'AND'), (1589, ' '), (1590, 'ILLUMINATE'), (1591, ' '), (1592, 'SOFT'), (1593, ' '), (1594, 'RADIANCE'), (1595, ' '), (1596, 'FOUNDATION'), (1597, ' '), (1598, 'SPF'), (1599, ' '), (1600, '50'), (1601, '/'), (1602, 'PA'), (1603, '++++'), (1604, ' '), (1605, 'มา'), (1606, 'ให้'), (1607, ' '), (1608, 'วันนี้'), (1609, 'ตั้งใจ'), (1610, 'จะ'), (1611, 'แต่งหน้า'), (1612, 'ด้วย'), (1613, ' '), (1614, 'TOM'), (1615, ' '), (1616, 'Ford'), (1617, ' '), (1618, 'ทั้งหมด'), (1619, ' '), (1620, 'งาน'), (1621, 'แป้ง'), (1622, ' '), (1623, 'งาน'), (1624, 'ลิป'), (1625, ' '), (1626, 'งาน'), (1627, 'ตา'), (1628, ' '), (1629, 'งาน'), (1630, 'คิ้ว'), (1631, ' '), (1632, 'เลย'), (1633, 'มา'), (1634, 'นั่ง'), (1635, 'รื้อ'), (1636, ' '), (1637, 'TOM'), (1638, ' '), (1639, 'FORD'), (1640, 'ออกมา'), (1641, 'บางส่วน'), (1642, ' '), (1643, 'เพิ่ง'), (1644, 'รู้'), (1645, 'ว่า'), (1646, 'ตัวเอง'), (1647, 'เป็น'), (1648, 'สาวก'), (1649, ' '), (1650, 'TOM'), (1651, ' '), (1652, 'FORD'), (1653, ' '), (1654, 'ก็'), (1655, 'วันนี้'), (1656, ' '), (1657, '😍😍😍😍'), (1658, 'สี'), (1659, 'ชัด'), (1660, 'ติด'), (1661, 'ทน'), (1662, 'เกลี่ย'), (1663, 'มาก'), (1664, 'ๆ'), (1665, 'ค่ะ'), (1666, ' '), (1667, 'ทิ้ง'), (1668, 'ไว้'), (1669, 'นาน'), (1670, 'ไม่'), (1671, 'แห้ง'), (1672, 'ด้วย'), (1673, ' '), (1674, 'บาง'), (1675, 'แบรนด์'), (1676, 'ทิ้ง'), (1677, 'ไว้'), (1678, 'ลืม'), (1679, 'ใช้'), (1680, 'นานๆ'), (1681, 'แห้ง'), (1682, 'จน'), (1683, 'ทา'), (1684, 'ไม่'), (1685, 'ได้'), (1686, 'เลย'), (1687, 'ค่ะ'), (1688, 'เคย'), (1689, 'ใช้'), (1690, ' '), (1691, 'set'), (1692, ' '), (1693, 'ทดลอง'), (1694, 'ค่ะ'), (1695, ' '), (1696, 'ดีงาม'), (1697, 'ค่ะ'), (1698, ' '), (1699, 'ส่วนตัว'), (1700, 'ผิว'), (1701, 'แห้ง'), (1702, ' '), (1703, 'ชุ่มชื่น'), (1704, 'ดี'), (1705, ' '), (1706, 'แต่'), (1707, 'วิธีใช้'), (1708, ' '), (1709, 'เยอะแยะ'), (1710, 'สิ่ง'), (1711, 'ไป'), (1712, 'มาก'), (1713, ' '), (1714, 'เรียง'), (1715, 'กัน'), (1716, ' '), (1717, '8'), (1718, ' '), (1719, 'ตัว'), (1720, ' '), (1721, 'เลย'), (1722, ' '), (1723, 'พัก'), (1724, 'ก๊อ'), (1725, 'นนน'), (1726, ' '), (1727, 'ขี้'), (1728, 'เกียด'), (1729, 'จง'), (1730, 'ข้าม'), (1731, 'ไป'), (1732, 'ยินดี'), (1733, 'แบ่งปัน'), (1734, 'สาว'), (1735, 'ๆ'), (1736, ' '), (1737, 'บ้าน'), (1738, 'จี'), (1739, 'บัน'), (1740, 'ทุกคน'), (1741, 'ค่า'), (1742, 'ขอ'), (1743, 'คำแนะนำ'), (1744, 'หน่อย'), (1745, 'คะ'), (1746, '\\u200b'), (1747, 'คือ'), (1748, 'เรา'), (1749, 'เป็น'), (1750, 'สิว'), (1751, 'เยอะ'), (1752, 'คะ'), (1753, '\\u200bเป็รอย'), (1754, 'ดำ'), (1755, 'ด้วย'), (1756, '\\u200b'), (1757, ' '), (1758, 'ต้อง'), (1759, 'เริ่ม'), (1760, 'ใช้'), (1761, 'แบบ'), (1762, 'ใหน'), (1763, 'ก่อน'), (1764, 'ตัว'), (1765, 'ใหน'), (1766, 'ก่อ'), (1767, 'นคะ'), (1768, '\\u200b'), (1769, ' '), (1770, 'ดู'), (1771, '\\u200b'), (1772, ' '), (1773, 'จขกท'), (1774, ' '), (1775, 'แล้ว'), (1776, '\\u200b'), (1777, ' '), (1778, 'มี'), (1779, 'สิว'), (1780, 'เหมือน'), (1781, 'เรา'), (1782, 'เลย'), (1783, 'ล้างมือ'), (1784, 'แทบตาย'), (1785, ' '), (1786, 'สุดท้าย'), (1787, 'เชื้อโรค'), (1788, 'เต็ม'), (1789, 'กระเป๋าสตางค์'), (1790, ' '), (1791, 'แบบนี้'), (1792, 'ไม่'), (1793, 'โอเค'), (1794, 'น้า'), (1795, 'าาาา'), (1796, 'น่ากลัว'), (1797, 'จัง'), (1798, 'ค่ะ'), (1799, ' '), (1800, 'มี'), (1801, 'อาการ'), (1802, 'ต้อง'), (1803, 'รีบ'), (1804, 'ไปหา'), (1805, 'หมอ'), (1806, 'เลย'), (1807, 'นะ'), (1808, 'ค่ะ'), (1809, ' '), (1810, 'เชื่อ'), (1811, 'ว่า'), (1812, ' '), (1813, 'คน'), (1814, 'ที่'), (1815, 'อาการ'), (1816, 'เเบบ'), (1817, 'คุณ'), (1818, 'ลุง'), (1819, ' '), (1820, 'ต้อง'), (1821, 'มี'), (1822, 'อีก'), (1823, 'แน่นอน'), (1824, ' '), (1825, 'เสียใจ'), (1826, 'กับ'), (1827, 'ครอบครัว'), (1828, 'ด้วย'), (1829, 'นะ'), (1830, 'ค่ะ'), (1831, 'เผื่อ'), (1832, 'สนใจ'), (1833, 'ปี'), (1834, '15'), (1835, 'รถ'), (1836, 'ผม'), (1837, 'ขาย'), (1838, 'เอง'), (1839, 'มือ'), (1840, 'แรก'), (1841, 'ป้าย'), (1842, 'แดง'), (1843, 'ไมล์'), (1844, '6'), (1845, 'หมื่น'), (1846, 'แท้'), (1847, 'เข้า'), (1848, 'ศูนย์'), (1849, 'ทุก'), (1850, 'ระยะ'), (1851, 'ครับ'), (1852, 'ไฟ'), (1853, 'แนน'), (1854, 'ไม่'), (1855, 'พัก'), (1856, 'ชำระหนี้'), (1857, 'ไป'), (1858, 'ร้องเรียน'), (1859, 'ใด้'), (1860, 'ที่ไหน'), (1861, 'ค่ะ'), (1862, 'ใคร'), (1863, 'รุ้'), (1864, 'มั่ง'), (1865, 'ขอบคุณ'), (1866, 'ค้าบ'), (1867, 'บบบ'), (1868, ' '), (1869, 'ฝาก'), (1870, 'กด'), (1871, 'สับ'), (1872, 'ตะไคร้'), (1873, ' '), (1874, 'แช'), (1875, 'แนล'), (1876, 'ยู'), (1877, 'ทู'), (1878, 'ป'), (1879, 'ของ'), (1880, 'เรา'), (1881, 'ด้วย'), (1882, 'น้า'), (1883, 'า😂'), (1884, 'ดู'), (1885, 'หน้าตา'), (1886, 'ไม่ค่อย'), (1887, 'เต็มใจ'), (1888, 'นะคะ'), (1889, ' '), (1890, 'เปลี่ยน'), (1891, 'อาชีพ'), (1892, 'ดีกว่า'), (1893, 'ไหม'), (1894, '?🤣'), (1895, 'ขออนุญาต'), (1896, 'ฝา'), (1897, 'กร้าน'), (1898, 'ค่ะ'), (1899, ' '), (1900, 'แป้ง'), (1901, 'ตลับ'), (1902, 'เอน'), (1903, 'วี่'), (1904, 'ตลับ'), (1905, 'ดำ'), (1906, 'เรา'), (1907, 'มี'), (1908, 'พร้อม'), (1909, 'ส่ง'), (1910, 'จ้า'), (1911, ' '), (1912, 'สนใจ'), (1913, 'ทัก'), (1914, 'มา'), (1915, 'ได้'), (1916, 'จ้า'), (1917, 'Line'), (1918, ':'), (1919, 'tuy'), (1920, '58'), (1921, 'ได้'), (1922, 'น้อง'), (1923, 'มา'), (1924, 'ใหม่'), (1925, ' '), (1926, 'ช่วย'), (1927, 'ตั้งชื่อ'), (1928, 'หน่อย'), (1929, 'ค่ะ'), (1930, ' '), (1931, '('), (1932, 'ด.ช.'), (1933, ')'), (1934, ' '), (1935, 'ขอ'), (1936, ' '), (1937, '2'), (1938, ' '), (1939, 'พยางค์'), (1940, ' '), (1941, 'ขึ้นต้น'), (1942, 'สระ'), (1943, '-'), (1944, 'โ'), (1945, ' '), (1946, 'ลงท้าย'), (1947, 'สระ'), (1948, ' '), (1949, '-'), (1950, 'า'), (1951, 'โดน'), (1952, 'กัน'), (1953, 'เป็น'), (1954, 'ทอด'), (1955, 'ๆ'), (1956, 'ค่ะ'), (1957, ' '), (1958, 'แอด'), (1959, 'ก็'), (1960, 'ปฎิ'), (1961, 'เส'), (1962, 'ธ'), (1963, 'งาน'), (1964, 'ช่วง'), (1965, 'นั้น'), (1966, 'หมด'), (1967, 'ทุก'), (1968, 'งาน'), (1969, 'เลย'), (1970, 'เหมือนกัน'), (1971, '><!'), (1972, ' '), (1973, 'ก็'), (1974, 'เสียโอกาส'), (1975, ' '), (1976, 'แต่'), (1977, 'ช่างมัน'), (1978, 'ค่ะ'), (1979, ' '), (1980, 'เอาเป็นว่า'), (1981, 'ถือว่า'), (1982, 'ดวง'), (1983, 'เรา'), (1984, 'จะ'), (1985, 'ต้อง'), (1986, 'เป็น'), (1987, 'แบบนี้'), (1988, 'ละ'), (1989, 'กัน'), (1990, 'ค่ะ'), (1991, 'ที่'), (1992, 'ออสเตรเลีย'), (1993, 'ตอนนี้'), (1994, 'บาง'), (1995, 'สายการบิน'), (1996, 'ให้'), (1997, 'พนักงาน'), (1998, 'มา'), (1999, 'ทำงาน'), (2000, 'เติม'), (2001, 'ของ'), (2002, 'ที่'), (2003, ' '), (2004, 'supermarket'), (2005, ' '), (2006, 'แล้ว'), (2007, 'ค่ะ'), (2008, 'น่ารัก'), (2009, 'หน้า'), (2010, 'เอ็นดู'), (2011, ' '), (2012, 'หน้ากาก'), (2013, 'เด็ก'), (2014, 'มัน'), (2015, 'หา'), (2016, 'ยาก'), (2017, 'อะ'), (2018, 'เนอะ'), (2019, 'รู'), (2020, 'กก'), (2021, 'กก'), (2022, 'กก'), (2023, 'ก'), (2024, 'พวก'), (2025, 'มะ'), (2026, 'นุด'), (2027, 'หายหัว'), (2028, 'ไป'), (2029, 'ไหน'), (2030, 'หมด'), (2031, ' '), (2032, 'เรา'), (2033, 'ยึด'), (2034, 'สนามบิน'), (2035, 'ไว้'), (2036, 'หมด'), (2037, 'แล้ว'), (2038, ' '), (2039, 'โลก'), (2040, 'ต้อง'), (2041, 'เป็น'), (2042, 'ของ'), (2043, 'เรา'), (2044, ' '), (2045, '😸'), (2046, 'ชอบ'), (2047, 'มาก'), (2048, 'ค่ะ'), (2049, 'สวย'), (2050, 'ค่ะ'), (2051, 'อยาก'), (2052, 'ยุ่'), (2053, 'แบบนี้'), (2054, 'นะ'), (2055, 'ค่ะ'), (2056, 'ขอบ'), (2057, 'คุ'), (2058, 'น'), (2059, 'เจ้า'), (2060, 'ง'), (2061, 'พา'), (2062, 'ภ'), (2063, 'มาก'), (2064, 'ค่ะ'), (2065, 'คนรวย'), (2066, 'มัน'), (2067, 'หวง'), (2068, 'เงิน'), (2069, 'จะ'), (2070, 'ตาย'), (2071, 'ไป'), (2072, ' '), (2073, 'กลัว'), (2074, 'ตัวเลข'), (2075, 'การ'), (2076, 'จัดลำดับ'), (2077, 'มหา'), (2078, 'เศษ'), (2079, 'ผี'), (2080, 'จะ'), (2081, 'ตกลง'), (2082, ' '), (2083, 'ปั่น'), (2084, 'เงิน'), (2085, 'กัน'), (2086, 'จน'), (2087, 'ตายห่า'), (2088, 'คา'), (2089, 'ธนบัตร'), (2090, 'นั่นแหละ'), (2091, ' '), (2092, 'มัน'), (2093, 'หวัง'), (2094, 'ขน'), (2095, 'เอา'), (2096, 'ไป'), (2097, 'ใช้'), (2098, 'ใน'), (2099, 'ปรโลก'), (2100, 'กัน'), (2101, 'ทั้งนั้น'), (2102, 'โรค'), (2103, 'นี้'), (2104, 'อาจจะ'), (2105, 'เคย'), (2106, 'เกิดขึ้น'), (2107, 'เมื่อ'), (2108, 'หมื่น'), (2109, 'ปี'), (2110, 'มา'), (2111, 'แล้วก็'), (2112, 'อาจ'), (2113, 'เป็นได้'), (2114, 'บาง'), (2115, 'ศพ'), (2116, 'ถูก'), (2117, 'เอา'), (2118, 'ไป'), (2119, 'เผา'), (2120, 'แต่'), (2121, 'บาง'), (2122, 'ศพ'), (2123, 'ถูก'), (2124, 'ฝั่ง'), (2125, 'เอาไว้'), (2126, 'แล้ว'), (2127, 'คน'), (2128, 'ยุค'), (2129, 'นี้'), (2130, 'มือบอน'), (2131, 'ไป'), (2132, 'ขุด'), (2133, 'เอา'), (2134, 'ศพ'), (2135, 'ที่'), (2136, 'ฝัง'), (2137, 'เอาไว้'), (2138, 'ขึ้น'), (2139, 'มา'), (2140, 'พร้อมกับ'), (2141, 'เชื้อโรค'), (2142, 'ตัว'), (2143, 'นี้'), (2144, 'ต่อ'), (2145, 'นี้'), (2146, 'ศพ'), (2147, 'ที่'), (2148, 'เผา'), (2149, 'เชื้อ'), (2150, 'ก็'), (2151, 'จะ'), (2152, 'ตาย'), (2153, 'แล้ว'), (2154, 'ศพ'), (2155, 'ที่'), (2156, 'ฝัง'), (2157, 'ถ้า'), (2158, 'ต่อไป'), (2159, 'อีก'), (2160, 'ร้อย'), (2161, 'ปี'), (2162, 'มี'), (2163, 'คน'), (2164, 'ไป'), (2165, 'ขุด'), (2166, 'ขึ้น'), (2167, 'มา'), (2168, 'แล้ว'), (2169, 'เจอ'), (2170, 'ถุง'), (2171, 'คงจะ'), (2172, 'ต้อง'), (2173, 'อยากรู้'), (2174, 'อย่า'), (2175, 'ก'), (2176, 'เห็น'), (2177, 'ว่า'), (2178, 'มัน'), (2179, 'เป็น'), (2180, 'อะไร'), (2181, 'เชื้อโรค'), (2182, 'จะ'), (2183, 'กลาย'), (2184, 'สาย'), (2185, 'พัน'), (2186, 'ก็'), (2187, 'จะ'), (2188, 'ระบาด'), (2189, 'ขึ้น'), (2190, 'มา'), (2191, 'อีก'), (2192, 'รฟม.'), (2193, ' '), (2194, 'ขอความร่วมมือ'), (2195, 'ผู้โดยสาร'), (2196, 'รถไฟฟ้า'), (2197, ' '), (2198, 'MRT'), (2199, ' '), (2200, 'กรอก'), (2201, 'แบบ'), (2202, 'คำถาม'), (2203, 'สุขภาพ'), (2204, ' '), (2205, '('), (2206, 'ต.'), (2207, '8'), (2208, ' '), (2209, 'คค'), (2210, ' '), (2211, ')'), (2212, ' '), (2213, 'เมื่อ'), (2214, 'เดิน'), (2215, 'ทางข้าม'), (2216, 'เขต'), (2217, 'พื้นที่'), (2218, 'จังหวัด'), (2219, 'เรื่อง'), (2220, 'นี้'), (2221, 'เรา'), (2222, 'วน'), (2223, 'เป็น'), (2224, '10'), (2225, 'รอบ'), (2226, ' '), (2227, 'หลง'), (2228, 'มา'), (2229, 'หลาย'), (2230, 'ปี'), (2231, 'ยัง'), (2232, 'หา'), (2233, 'ทางออก'), (2234, 'ไม่'), (2235, 'ได้'), (2236, 'เลย'), (2237, 'ยย'), (2238, 'ค่ะ'), (2239, ' '), (2240, 'ใน'), (2241, 'ยู'), (2242, 'ทู'), (2243, 'ป'), (2244, 'ก็'), (2245, 'มี'), (2246, ' '), (2247, 'ว่า'), (2248, 'แล้วก็'), (2249, 'ไป'), (2250, 'วน'), (2251, 'อีก'), (2252, 'ซัก'), (2253, 'รอบ'), (2254, 'ดีกว่า'), (2255, '555555'), (2256, 'ตอนนี้'), (2257, 'กำลัง'), (2258, 'เริ่ม'), (2259, 'ดู'), (2260, 'เลขา'), (2261, 'คิม'), (2262, ' '), (2263, 'มา'), (2264, 'สอง'), (2265, 'ตอนแรก'), (2266, 'ดีมาก'), (2267, 'กก'), (2268, 'ไป'), (2269, 'สอย'), (2270, 'มา'), (2271, 'มะ'), (2272, 'วาน'), (2273, ' '), (2274, 'ราคา'), (2275, 'นี้'), (2276, 'จริงๆ'), (2277, 'ๆ'), (2278, ' '), (2279, 'เสียดาย'), (2280, 'มี'), (2281, 'เเต่'), (2282, '32'), (2283, 'g'), (2284, ' '), (2285, 'รายละเอียด'), (2286, 'เท่าที่'), (2287, 'ทราบ'), (2288, 'อยู่'), (2289, 'ใน'), (2290, 'คอมเม้นต์'), (2291, 'ค่ะ'), (2292, ' '), (2293, 'ตอบ'), (2294, 'ไป'), (2295, 'ส่วน'), (2296, 'นึง'), (2297, 'เเล้ว'), (2298, ' '), (2299, 'ไม่ต้อง'), (2300, 'ทัก'), (2301, ' '), (2302, 'มา'), (2303, 'ถาม'), (2304, 'ส่วนตัว'), (2305, 'นะคะ'), (2306, 'ชอบ'), (2307, 'คำ'), (2308, 'เคลม'), (2309, 'ตรง'), (2310, 'เงินเดือน'), (2311, 'มาก'), (2312, 'ค่ะ'), (2313, ' '), (2314, 'ถ้า'), (2315, 'ทำงาน'), (2316, 'ได้'), (2317, 'ตามที่'), (2318, 'พูด'), (2319, ' '), (2320, 'เท่าไหร่'), (2321, 'ก็'), (2322, 'จ่าย'), (2323, 'ได้'), (2324, 'หมด'), (2325, '😆👏👏'), (2326, 'พาย'), (2327, 'เปี๊ยะ'), (2328, 'กรอบ'), (2329, ' '), (2330, 'ไส้'), (2331, 'หมูหยอง'), (2332, 'พริก'), (2333, 'เผา'), (2334, 'ค่ะ'), (2335, ' '), (2336, 'กล่อง'), (2337, '100'), (2338, ' '), (2339, 'บาท'), (2340, '7'), (2341, 'ชิ้น'), (2342, ' '), (2343, 'ใช้'), (2344, 'เนยสด'), (2345, 'หอม'), (2346, 'อร่อย'), (2347, 'ๆ'), (2348, 'อี'), (2349, 'เหี้ย'), (2350, 'เอ๊ย'), (2351, 'ยยยยย'), (2352, ' '), (2353, 'ตอนแรก'), (2354, 'ก็'), (2355, 'ว่า'), (2356, 'หล่อ'), (2357, 'แล้ว'), (2358, 'นะ'), (2359, ' '), (2360, 'ปลื้ม'), (2361, 'แล้ว'), (2362, 'นะ'), (2363, ' '), (2364, 'มา'), (2365, 'เจอ'), (2366, 'ชอท'), (2367, 'เล่น'), (2368, 'กีตาร์'), (2369, 'คือ'), (2370, 'ออ'), (2371, 'ออ'), (2372, 'ออ'), (2373, 'บวก'), (2374, 'ไป'), (2375, 'เลย'), (2376, 'แม่'), (2377, 'พัน'), (2378, 'พัน'), (2379, 'ล้าน'), (2380, 'คะแนน'), (2381, ' '), (2382, 'เอา'), (2383, 'ไป'), (2384, 'เลย'), (2385, 'ทั้ง'), (2386, 'ใจ'), (2387, ' '), (2388, 'ทั้งตัว'), (2389, ' '), (2390, 'ทั้ง'), (2391, 'สมุด'), (2392, 'บัญ'), (2393, ' '), (2394, 'เอา'), (2395, 'ไป'), (2396, 'เล้ย'), (2397, 'ยยยยยยยย'), (2398, 'รอบ'), (2399, 'หน้า'), (2400, 'เชิญ'), (2401, 'คุณ'), (2402, 'ทิม'), (2403, 'มา'), (2404, 'หน่อย'), (2405, 'นะ'), (2406, 'แม่'), (2407, 'มี'), (2408, 'ใคร'), (2409, 'อ่าน'), (2410, 'เรื่อง'), (2411, 'นี้'), (2412, 'บ้าง'), (2413, ' '), (2414, 'สนุก'), (2415, 'มา'), (2416, 'กก'), (2417, 'กก'), (2418, 'กก'), (2419, 'กก'), (2420, ' '), (2421, 'จน'), (2422, 'ต้อง'), (2423, 'ซื้อ'), (2424, 'เก็บ'), (2425, 'น้อง'), (2426, 'ก็'), (2427, 'ไม่'), (2428, 'กล้า'), (2429, 'ลง'), (2430, 'แข่ง'), (2431, 'พี่'), (2432, 'ท็อฟ'), (2433, ' '), (2434, 'เพราะ'), (2435, 'เรา'), (2436, 'ยัง'), (2437, 'ไม่'), (2438, 'เก่ง'), (2439, 'เหมือนกัน'), (2440, ' '), (2441, 'และ'), (2442, ' '), (2443, 'Toxic'), (2444, ' '), (2445, 'Comment'), (2446, 'ข้าวผัด'), (2447, 'เนย'), (2448, 'กระเทียม'), (2449, 'กับ'), (2450, 'เครื่องปรุง'), (2451, 'หอม'), (2452, 'ๆ'), (2453, ' '), (2454, 'ทาน'), (2455, 'คู่'), (2456, 'กับ'), (2457, 'เนื้อ'), (2458, 'สไลด์'), (2459, 'ราด'), (2460, 'ซอส'), (2461, 'ฉ่ำ'), (2462, 'ๆ'), (2463, 'อร่อย'), (2464, 'เหาะ'), (2465, 'ใน'), (2466, 'ราคา'), (2467, 'มิตรภาพ'), (2468, ' '), (2469, 'ตุ๊กแก'), (2470, 'รับ'), (2471, 'ทำ'), (2472, 'ข้าว'), (2473, 'กล่อง'), (2474, 'ด้วย'), (2475, 'นะ'), (2476, 'ฮะ'), (2477, 'สั่ง'), (2478, 'เยอะ'), (2479, 'ลด'), (2480, 'จ้า'), (2481, 'าาาา'), (2482, 'อุปกรณ์'), (2483, 'เครื่อง'), (2484, 'ออกกำลังกาย'), (2485, 'ที่'), (2486, 'สวนสาธารณะ'), (2487, ' '), (2488, 'ราวบันได'), (2489, ' '), (2490, 'ราว'), (2491, 'สะพาน'), (2492, ' '), (2493, 'ด้วย'), (2494, 'ซิ'), (2495, 'คับ'), (2496, '..ขอ'), (2497, 'อนุ'), (2498, 'ญาติ'), (2499, 'นำ'), (2500, 'ภาพ'), (2501, 'ไป'), (2502, 'เป็น'), (2503, 'ปก'), (2504, ' '), (2505, 'fb'), (2506, ' '), (2507, 'ได้'), (2508, 'มั้ย'), (2509, 'คะ'), (2510, ' '), (2511, 'ชอบ'), (2512, 'ภาพ'), (2513, 'สวย'), (2514, 'มาก'), (2515, 'เลย'), (2516, ' '), (2517, 'ชอบ'), (2518, 'ดู'), (2519, 'ทุ่ง'), (2520, 'บัว'), (2521, 'ค่ะ'), (2522, 'เสียดาย'), (2523, 'จัง'), (2524, ' '), (2525, 'มะนาว'), (2526, 'ไม่'), (2527, 'มี'), (2528, 'น้ำ'), (2529, 'รด'), (2530, ' '), (2531, 'ลูก'), (2532, 'หล่น'), (2533, 'หมด'), (2534, 'เลย'), (2535, ' '), (2536, 'ตอนนี้'), (2537, 'โล'), (2538, 'เป็น'), (2539, 'ร้อย'), (2540, ' '), (2541, 'ถ้า'), (2542, 'ไม่'), (2543, 'หล่น'), (2544, 'คง'), (2545, 'ได้'), (2546, 'หลาย'), (2547, 'ตังค์'), (2548, ' '), (2549, 'ฝน'), (2550, 'เพิ่ง'), (2551, 'ตก'), (2552, 'เมื่อวาน'), (2553, 'ครับ'), (2554, 'ออกมา'), (2555, 'พ่น'), (2556, 'ที่'), (2557, 'ตลาด'), (2558, 'สิน'), (2559, 'ทอง'), (2560, 'หน้า'), (2561, 'รพ.'), (2562, 'พระ'), (2563, 'นั่ง'), (2564, 'เกล้า'), (2565, 'บ้าง'), (2566, 'ซิ'), (2567, '🥺🥺🥺🥺'), (2568, 'แฟนคลับ'), (2569, 'หงส์แดง'), (2570, 'ชาวไทย'), (2571, ' '), (2572, 'ฝาก'), (2573, 'ข้อความ'), (2574, 'ไป'), (2575, 'ถึง'), (2576, 'แฟนคลับ'), (2577, 'หงส์แดง'), (2578, 'ทั่ว'), (2579, 'โล'), (2580, 'กค'), (2581, 'รัฟ'), (2582, 'ดิน'), (2583, 'หมัก'), (2584, 'ยัง'), (2585, 'ไม่'), (2586, 'ดี'), (2587, 'พอ'), (2588, 'ค่ะ'), (2589, 'ยัง'), (2590, 'เป็น'), (2591, 'แห้ง'), (2592, 'แก'), (2593, 'รน'), (2594, 'ๆ'), (2595, ' '), (2596, 'หา'), (2597, 'ความร่วน'), (2598, 'ซุย'), (2599, 'ไม่'), (2600, 'ได้'), (2601, 'ต้นกล้า'), (2602, 'เลย'), (2603, 'เหลือง'), (2604, 'แก'), (2605, 'รน'), (2606, 'ๆ'), (2607, 'ค่ะ'), (2608, 'ก่อน'), (2609, 'ส่ง'), (2610, 'เขา'), (2611, 'ฟอง'), (2612, 'ละ'), (2613, '4.'), (2614, 'ขาย'), (2615, 'ยาก'), (2616, 'ด้วย'), (2617, 'ตอนนี้'), (2618, 'ส่ง'), (2619, '5.'), (2620, 'บ.'), (2621, 'ไม่'), (2622, 'พอ'), (2623, 'ส่ง'), (2624, 'แปลก'), (2625, 'เหมือนกัน'), (2626, 'ถ้า'), (2627, 'เรา'), (2628, 'เป็น'), (2629, 'รัฐบาล'), (2630, ' '), (2631, 'ก้อ'), (2632, 'เหนื่อย'), (2633, 'นะ'), (2634, ' '), (2635, 'เชื้อโรค'), (2636, 'เอง'), (2637, ' '), (2638, 'ไฟป่า'), (2639, 'เอง'), (2640, ' '), (2641, 'ไฟป่า'), (2642, 'นี่'), (2643, 'ไม่'), (2644, 'รู้'), (2645, 'คน'), (2646, 'เผา'), (2647, 'หา'), (2648, 'ของป่า'), (2649, 'อะป่าว'), (2650, 'อันนี้'), (2651, 'ถาม'), (2652, 'จริงจัง'), (2653, ' '), (2654, 'หา'), (2655, 'แอลกอฮอล์'), (2656, 'ยากเย็น'), (2657, ' '), (2658, 'ใช้'), (2659, 'เหล้าขาว'), (2660, ' '), (2661, 'ว๊อดก้า'), (2662, ' '), (2663, 'ยิน'), (2664, ' '), (2665, 'อะไร'), (2666, 'ทำนอง'), (2667, 'นี้'), (2668, ' '), (2669, 'พอ'), (2670, 'ปะ'), (2671, 'ทัง'), (2672, ' '), (2673, 'ได้'), (2674, 'มั้ย'), (2675, ' '), (2676, 'มัน'), (2677, 'สามารถ'), (2678, 'ฆ่าเชื้อ'), (2679, 'ได้'), (2680, 'บ้าง'), (2681, 'มั้ย'), (2682, 'คะ'), (2683, ' '), (2684, 'ไม่'), (2685, 'ได้'), (2686, 'กวน'), (2687, 'จริงๆ'), (2688, 'นะคะ'), (2689, 'อิคน'), (2690, 'นี้'), (2691, 'เลย'), (2692, 'ค่ะ'), (2693, 'จ้อง'), (2694, 'จะ'), (2695, 'ขาย'), (2696, 'ครอส'), (2697, 'อาหารเสริม'), (2698, 'มัง'), (2699, 'สา'), (2700, 'วิรัติ'), (2701, 'แพง'), (2702, 'ๆ'), (2703, 'ล่า'), (2704, 'สุดคน'), (2705, 'แชร์'), (2706, 'คลิป'), (2707, ' '), (2708, 'โค'), (2709, 'วิด'), (2710, 'ไม่'), (2711, 'น่ากลัว'), (2712, 'ไป'), (2713, 'เป็น'), (2714, 'หมื่น'), (2715, 'แชร์'), (2716, ' '), (2717, 'นาง'), (2718, 'บอก'), (2719, 'กิน'), (2720, 'ข้าวโอ๊ต'), (2721, 'กิน'), (2722, 'ผัก'), (2723, ' '), (2724, 'อย่า'), (2725, 'กิน'), (2726, 'เนื้อ'), (2727, 'ร่างกาย'), (2728, 'จะ'), (2729, 'ได้'), (2730, 'แข๊ง'), (2731, 'แรง'), (2732, ' ')]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from pythainlp.tokenize import word_tokenize\n",
    "from deepcut import DeepcutTokenizer\n",
    "from pythainlp import sent_tokenize, word_tokenize\n",
    "from pythainlp.util import normalize\n",
    "import glob\n",
    "\n",
    "def get_master_count(file_path, sep):\n",
    "    str = ''\n",
    "    master_list = []\n",
    "    for fn in glob.glob(file_path):\n",
    "        with open (fn) as f:\n",
    "            original = f.readlines()\n",
    "            lines = [line.rstrip('\\n') for line in original]\n",
    "            for each_line in lines: \n",
    "                str = str + each_line\n",
    "            master_list.extend(str.split(sep))\n",
    "\n",
    "    clean_list = []\n",
    "    for w in master_list:\n",
    "        if w.isspace() == False or len(w) == 0:\n",
    "            clean_list.append(w)\n",
    "            \n",
    "    master_wcount = len(clean_list)\n",
    "    return master_wcount\n",
    "    \n",
    "    \n",
    "master_count = get_master_count('MasterCut/*', '/')\n",
    "master_sent_count = get_master_count('Sent_MasterCut/*', '_#_')\n",
    "\n",
    "print ('Master Word Count: ', master_count)\n",
    "print('Master Sent Count: ', master_sent_count)\n",
    "raw_str = ''\n",
    "for fn in glob.glob('raw/*'):\n",
    "    with open (fn) as f:\n",
    "        train = f.readlines()\n",
    "        lines = [line.rstrip('\\n') for line in train]\n",
    "        for each_line in lines: \n",
    "            raw_str = raw_str + each_line\n",
    "print(raw_str)     \n",
    "print(len(raw_str))\n",
    "import time\n",
    "start_time_newmm = time.time()\n",
    "newmm = word_tokenize(raw_str, engine=\"newmm\", keep_whitespace = True)\n",
    "sent_tokens= sent_tokenize(raw_str, engine=\"newmm\")\n",
    "# print(sent_tokens)\n",
    "# print(len(sent_tokens))\n",
    "end_time_newmm = time.time()\n",
    "\n",
    "start_time_deepcut = time.time()\n",
    "deepcut = word_tokenize(raw_str, engine=\"deepcut\",keep_whitespace = True)\n",
    "end_time_deepcut = time.time()\n",
    "# print (len(sent_tokens_newmm))\n",
    "# print(sent_tokens_newmm)\n",
    "# print(\"sent_tokenize (newmm):\",  (len(sent_tokens_newmm)))\n",
    "#print(\"sent_tokenize (attacut):\",  (len(sent_tokens_attacut)))\n",
    "# print(\"sent_tokenize (Deepcut):\",  (sent_tokens_deepcut))\n",
    "\n",
    "#print ('Master Word Count: ', master_count)                          \n",
    "# print ('NewMM Word count: ', len(newmm))\n",
    "# print ('DeepcutWord count: ', len(deepcut))\n",
    "\n",
    "# print('time spent using NewMM:', end_time_newmm - start_time_newmm)\n",
    "# print('time spent using deepcut:', end_time_deepcut - start_time_deepcut)\n",
    "\n",
    "    \n",
    "# presidents = [\"Washington\", \"Adams\", \"Jefferson\", \"Madison\", \"Monroe\", \"Adams\", \"Jackson\"]\n",
    "# for num, name in enumerate(presidents, start=1):\n",
    "#     print(\"President {}: {}\".format(num, name))\n",
    "\n",
    "#     Compare NEWMM VS Deepcut side by side\n",
    "    \n",
    "# print(\"newmm\" + '\\t\\t\\t'+  \"deepcut\")\n",
    "newmm_enu = list(enumerate(newmm,1))\n",
    "print(newmm_enu)  \n",
    "deep_enu = list(enumerate(deepcut, 1))\n",
    "#print('Deepcut ',deep_enu)  \n",
    "# ''' pad the two results so they have equal length\n",
    "# max_length = max([len(sequence) for sequence in batch])\n",
    "#     for i in range(len(batch)):\n",
    "#         batch[i] += ['</s>'] * (max_length - len(batch[i]))'''\n",
    "# for ((newmm_idx, newmm_word), (deep_idx, deep_word)) in zip (newmm_enu, deep_enu):\n",
    "#     print (newmm_idx, newmm_word + '/t/t/t'+ deep_idx , deep_word)\n",
    "\n",
    "# master_list =\n",
    "# master_enu = enumerate(master_list,1):\n",
    "#     print (i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NewMM ['ไฟแนนซ์']\n",
      "Deepcut ['ไฟแนนซ์']\n"
     ]
    }
   ],
   "source": [
    "from pythainlp.tokenize import word_tokenize\n",
    "from deepcut import DeepcutTokenizer  \n",
    "text = 'ไฟแนนซ์'\n",
    "deepcut = word_tokenize(text, engine=\"deepcut\")\n",
    "newmm = word_tokenize(text, engine=\"newmm\")\n",
    "print ('NewMM', newmm)\n",
    "#print (*newmm, sep = \"\\n\")\n",
    "print ('Deepcut', deepcut)\n",
    "#print (*deepcut, sep = \"\\n\")\n",
    "#print (len(deepcut))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compare w difference btw master and engines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.8192875760208513\n"
     ]
    }
   ],
   "source": [
    "from attacut import tokenize, Tokenizer\n",
    "from pythainlp.tokenize import word_tokenize\n",
    "from deepcut import DeepcutTokenizer\n",
    "from pythainlp import sent_tokenize, word_tokenize\n",
    "from pythainlp.util import normalize\n",
    "import glob\n",
    "from pythainlp.tokenize import word_tokenize\n",
    "from deepcut import DeepcutTokenizer \n",
    "\n",
    "'''function to extract the diffent results from all results          \n",
    "y1: output from master\n",
    "y2: output from newmm\n",
    "\n",
    "y1a, y2a = align(y1, y2)\n",
    "len(y1a) == len(y2a)\n",
    "\n",
    "y1 = ['the', 'good', 'book', 'was', 'cheap']\n",
    "y2 = ['the', 'goodbook', 'was', 'cheap']\n",
    "\n",
    "y1a, y2a = align(y1, y2)\n",
    "\n",
    "y1a = ['the', 'good', 'book', 'was', 'cheap']\n",
    "y2a = ['the', 'goodbook', '_', 'was', 'cheap']\n",
    "\n",
    "#element-wise comparison, i.e.\n",
    "for i in range(len(y1a)):\n",
    "    if y1a[i] != y2a[i]:\n",
    "        print(y1a[i], y2a[i])\n",
    "\n",
    "# output:\n",
    "# good , goodbook\n",
    "# book, _\n",
    "'''\n",
    "\n",
    "def diff(li1, li2): \n",
    "    return list(set(li1).symmetric_difference(set(li2)))\n",
    "#[i for i, j in zip(li1, li2) if i == j] \n",
    "#https://intellipaat.com/community/11860/how-can-i-compare-two-lists-in-python-and-return-matches\n",
    "\n",
    "#https://stackoverflow.com/questions/3462143/get-difference-between-two-lists\n",
    "\n",
    "def score(master,data,csv_file_path):\n",
    "    tp = len(master) - len(diff(data,master))\n",
    "    result = get_fn_fp_csv_file(csv_file_path)\n",
    "    fn = result[0]\n",
    "    fp = result[1]\n",
    "    # precision\n",
    "    precision = tp/(tp+fp)\n",
    "    recall = tp/(tp+fn)\n",
    "    # recall\n",
    "    F1 = 2*((precision*recall)/(precision+recall)) # average of precision and recall; harmonic mean (as opposed to arithmetic mean or geometric mean)\n",
    "    return F1\n",
    "\n",
    "# input => เดอะ/พร็อพส์/ /กว่า/ /15/ /ปี/บน/เส้นทาง/ตัวแทน/เช่า/ /-/ /ซื้อ/อสังหาริมทรัพย์/สร้าง/เมือง/ไทย/เป็น/ที่อยู่/ใน/ฝัน/ของ/ /Expat/ /เอลิสต์/ทั่ว/โลก/\n",
    "# output => ['ตอน', 'นี้', 'อยาก', 'บอก', 'ว่า', 'น้ำหนัก', 'กู', 'ทะลุ', 'เพดาน', 'สูง', 'กว่า', 'กราฟ', 'ตลาด', 'หุ้น', 'มาก', 'ตอนนี้']\n",
    "def read_master_file(file_path,delimeter=None):  #BOKE\n",
    "    raw_str = ''\n",
    "    for fn in glob.glob(file_path):\n",
    "        with open (fn) as f:\n",
    "            original = f.readlines()\n",
    "            lines = [line.rstrip('\\n') for line in original]\n",
    "            for each_line in lines: \n",
    "                raw_str = raw_str + each_line\n",
    "            if delimeter is not None:\n",
    "                master_list = []\n",
    "                master_list.extend(str.split(delimeter))\n",
    "                return master_list\n",
    "    return raw_str\n",
    "\n",
    "def get_fn_fp_csv_file(file_path):\n",
    "    import csv\n",
    "    with open(file_path, mode='r') as csv_file:\n",
    "        csv_reader = csv.DictReader(csv_file)\n",
    "        next(csv_reader, None)  # skip the headers\n",
    "        fncount = 0\n",
    "        fpcount = 0\n",
    "        for row in csv_reader:\n",
    "#            print(row)\n",
    "#             print(\"row['FP']:\",row['FP'],type(row['FP']))\n",
    "            if row['FN'] != '':\n",
    "                try:\n",
    "                    fncount += int(row['FN']) # column index 1 (2nd column)\n",
    "                except ValueError:\n",
    "                    pass\n",
    "            if row['FP'] != '':\n",
    "                try:\n",
    "                    fpcount += int(row['FP']) # column index 2 (3rd column)\n",
    "                except ValueError:\n",
    "                    pass\n",
    "    return fncount, fpcount\n",
    "            \n",
    "raw_str = read_master_file('raw/*')\n",
    "newmm = word_tokenize(raw_str, engine=\"newmm\", keep_whitespace = False)\n",
    "\n",
    "# master_count = get_master_count('MasterCut/*', '/')\n",
    "# master_sent_count = get_master_count('Sent_MasterCut/*', '_#_')\n",
    "master_list = read_master_file('MasterCut/*','/')\n",
    "# print(master_list)\n",
    "# newmm_diff = Diff(newmm, master_list)### How to extract only the different output from newmm, not from both arg\n",
    "#print (' Master VS NewMM: ', len(newmm_diff))\n",
    "\n",
    "sc = score(master_list,newmm,'newmm_fnfp.csv')\n",
    "print(sc)\n",
    "\n",
    "# print (newmm_diff)\n",
    "# print ('_____________________________________________________________')\n",
    "# deepcut_diff = Diff(mastercut_list, deepcut)\n",
    "# print ('Master VS Deepcut ', len(deepcut_diff))\n",
    "#print (deepcut_diff)\n",
    "# print ('_____________________________________________________________')\n",
    "# engines_diff = Diff(newmm, deepcut)\n",
    "# print ('NewMM VS Deepcut ', len(engines_diff))\n",
    "#print (engines_diff)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# F1 Measure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def acc(ref,data)\n",
    "    F1 = 2*((precision*recall)/(precision+recall))\n",
    "    return F1\n",
    "\n",
    "''' \n",
    "Precision = tp/(tp+fp)\n",
    "Recall = tp/(tp+fn)\n",
    "\n",
    "find acc for \n",
    "1) word seg  \n",
    "Master VS NewMM:  521\n",
    "Master VS Deepcut  350\n",
    "\n",
    "Master Word Count:  3989\n",
    "NewMM Word count:  2347\n",
    "DeepcutWord count:  2589\n",
    "\n",
    "2) sent seg: Sent_tokens 33\n",
    "3) spell check - only 8/41 are corrected. what are FP and FN?\n",
    "4) NER\n",
    "\n",
    "'''\n",
    "\n",
    "w_newmm = acc()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check and Correct Spelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'slangs' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-5357a0040cdb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     27\u001b[0m ]\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m \u001b[0mprint\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mslangs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m \u001b[0;31m# for word in mispelled_words:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;31m#     print(\"{} -> {}\".format(word, correct(word)))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'slangs' is not defined"
     ]
    }
   ],
   "source": [
    "from pythainlp.spell import correct\n",
    "\n",
    "mispelled_words = [\n",
    "'โรงบาล',\n",
    "'จิง',\n",
    "'ขี้เกียด',\n",
    "'ใหน',\n",
    "'เเบบ',\n",
    "'ใด้',\n",
    "'รุ้',\n",
    "'หน้าเอ็นดู',\n",
    "'มะนุด',\n",
    "'ยุ่', \n",
    "'ขอบคุน',\n",
    "'เจ้างพาภ',\n",
    "'ฝั่ง,'\n",
    "'สายพัน',\n",
    "'มะวาน',\n",
    "'เเล้ว',\n",
    "'ซิคับ',\n",
    "'อนุญาติ',\n",
    "'ครัฟ',\n",
    "'แกรน',\n",
    "'มังสาวิรัติ',\n",
    "'แข๊งแรง',\n",
    "'เป็'\n",
    "]\n",
    "\n",
    "\n",
    "for word in mispelled_words:\n",
    "    print(\"{} -> {}\".format(word, correct(word)))\n",
    "    print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "แข็ง\n"
     ]
    }
   ],
   "source": [
    "from pythainlp.spell import spell\n",
    "from pythainlp.spell import NorvigSpellChecker\n",
    "\n",
    "\n",
    "a = ('โรงบาล')\n",
    "b = ('จิง')  # ['จิง']\n",
    "c = ('ขี้เกียด') #['ขี้เกียจ']\n",
    "d = ('ใหน') # ['ใหน']\n",
    "e = ('เเบบ') #['เบบ']\n",
    "f = ('ใด้') #['ใด้']\n",
    "g = ('รุ้') #['รู้', 'รุม', 'รุก', 'รุ', 'รุ้ง', 'ผุ้', 'รุด', 'รุณ', 'รุน', 'รุท', 'รุส', 'รุง', 'รี้', 'รุจ', 'รุป', 'รุย', 'รุต', 'รุฬ', 'รุธ', 'รุ้น', 'รุค', 'กุ้', 'รุ่', 'กรุ้', 'ตุ้', 'รุบ', 'รุซ', 'รุล', 'สุ้', 'อรุ้']\n",
    "h = ('หน้าเอ็นดู') #['หน้าเอ็นดู']\n",
    "i = ('มะนุด') #['สะดุด', 'มะนาว', 'มุด', 'ระนาด', 'ละมุด', 'มนุษ', 'มนุส', 'มะริด', 'ทะนุ', 'มนุ', 'มะรุม', 'มะนัง', 'มานุษ', 'มะหาด', 'มนุญ', 'นุด', 'หนุด', 'อนุด']\n",
    "j = ('แข๊ง') #['ยุ่']\n",
    "\n",
    "# 'ขอบคุน', 'เจ้างพาภ', 'ฝั่ง', 'สายพัน', 'มะวาน', 'เเล้ว', 'ซิคับ', 'อนุญาติ', \n",
    "# 'ครัฟ', 'แกรน', 'มังสาวิรัติ', 'แข๊งแรง', 'เป็')'''\n",
    "\n",
    "#x = spell(h, engine= \"pn\")\n",
    "x = NorvigSpellChecker()\n",
    "y = x.correct(j)\n",
    "\n",
    "\n",
    "print(y)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# add Custom dict\n",
    "\n",
    "evaluate NER tokens before and after running NER tagger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['คุณ', 'ณัฐชานันท์', ' ', 'จันทรานุวัฒน์']\n"
     ]
    }
   ],
   "source": [
    "from attacut import tokenize, Tokenizer\n",
    "from pythainlp.tokenize import word_tokenize\n",
    "from deepcut import DeepcutTokenizer\n",
    "from pythainlp import sent_tokenize, word_tokenize\n",
    "from pythainlp.util import normalize\n",
    "import glob\n",
    "from pythainlp.corpus.common import thai_words\n",
    "from pythainlp.tokenize import dict_trie, word_tokenize\n",
    "\n",
    "\n",
    "\n",
    "text = 'คุณณัฐชานันท์ จันทรานุวัฒน์'\n",
    "custom_dict_japanese_name = set(thai_words())\n",
    "g = custom_dict_japanese_name.add('ณัฐชานันท์')\n",
    "r = custom_dict_japanese_name.add('จันทรานุวัฒน์')\n",
    "trie = dict_trie(dict_source=custom_dict_japanese_name)\n",
    "h = word_tokenize(text, engine=\"newmm\", custom_dict=trie)\n",
    "                                    \n",
    "print (h)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# This function normalize thai text with normalizing rules as follows:\n",
    "\n",
    "Remove redudant symbol of tones and vowels.\n",
    "\n",
    "Subsitute [“เ”, “เ”] to “แ”.\n",
    "limit to only vowel, not text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "นานๆๆๆๆ\n"
     ]
    }
   ],
   "source": [
    "from pythainlp.util import normalize\n",
    "x = normalize('นานๆๆๆๆ')\n",
    "print (x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sent tokenization\n",
    "This function does not yet automatically recognize when a sentence actually ends. Rather it helps split text where white space and a new line is found. Options for engine\n",
    "whitespace+newline (default) - split by whitespace token and newline.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pythainlp.tokenize import word_tokenize\n",
    "from deepcut import DeepcutTokenizer\n",
    "from pythainlp import sent_tokenize, word_tokenize\n",
    "from pythainlp.util import normalize\n",
    "import glob\n",
    "def get_master_count(file_path, sep):\n",
    "    str = ''\n",
    "    master_list = []\n",
    "    for fn in glob.glob(file_path):\n",
    "        with open (fn) as f:\n",
    "            original = f.readlines()\n",
    "            lines = [line.rstrip('\\n') for line in original]\n",
    "            for each_line in lines: \n",
    "                str = str + each_line\n",
    "            master_list.extend(str.split(sep))\n",
    "\n",
    "#     clean_list = []\n",
    "#     for w in master_list:\n",
    "#         if w.isspace() == False or len(w) == 0:\n",
    "#             clean_list.append(w)\n",
    "            \n",
    "#     master_wcount = len(clean_list)\n",
    "    return master_list\n",
    "    \n",
    "#master_count = get_master_count('MasterCut/*', '/')\n",
    "master_sent_count = get_master_count('Sent_MasterCut/*', '_#_')\n",
    "\n",
    "#print ('Master Sent Count: ', master_sent_count)    \n",
    "raw_str = ''\n",
    "for fn in glob.glob('raw/*'):\n",
    "    with open (fn) as f:\n",
    "        train = f.readlines()\n",
    "        lines = [line.rstrip('\\n') for line in train]\n",
    "        for each_line in lines: \n",
    "            raw_str = raw_str + each_line\n",
    "            \n",
    "sent_tokens = sent_tokenize(raw_str, engine=\"whitespace\")\n",
    "# print('Sent_tokens', len(sent_tokens))\n",
    "# print(sent_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pythainlp.tag import pos_tag\n",
    "x = ['www.facebook.com/thepropsthailand']\n",
    "y = pos_tag(x, corpus='orchid_ud')\n",
    "#print (y)\n",
    "\n",
    "\n",
    "import glob\n",
    "from pythainlp.tag.named_entity import ThaiNameTagger\n",
    "raw_str = ''\n",
    "for fn in glob.glob('raw/*'):\n",
    "    with open (fn) as f:\n",
    "        train = f.readlines()\n",
    "        lines = [line.rstrip('\\n') for line in train]\n",
    "        for each_line in lines: \n",
    "            raw_str = raw_str + each_line\n",
    "\n",
    "ner1 = ThaiNameTagger()\n",
    "ner2 = ner1.get_ner(raw_str,pos=True)\n",
    "#print (ner2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SPELL CHECK need a def"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['โละ', 'นะจ๊ะ', 'นะ', 'จ๊ะ', 'เป็น', 'โรงพยาบาล', 'จริง', 'พักก่อน', 'ขี้เกียจ', 'นะ', 'ไฟแนนซ์', 'ได้', 'ไหน', 'รู้', 'ครับ', 'นะ', 'จ๊ะ', 'น่าเอ็นดู', 'ลูก', 'มนุษย์', 'อยู่', 'ขอบคุณ', 'นะคะ', 'เจ้าภาพ', 'ฝัง', 'สายพันธุ์', 'เลย', 'มาก', 'เมื่อวาน', 'เอ๊ย', 'คือ', 'เลย', 'มาก', 'จ๊ะ', 'สิ', 'ครับ', 'อนุญาต', 'ครับ', 'แกร็น', 'มังสวิรัติ', 'แข็งแรง', '', '']\n",
      "โล๊ะ -> โล๊ะ\n",
      "น่าจ้า -> ค่าจ้าง\n",
      "น่า -> น่า\n",
      "จ้าาา -> จ้าาา\n",
      "เปน -> เปน\n",
      "โรงบาล -> โรงงาน\n",
      "จิง -> จิง\n",
      "พักก๊อนนน -> พักก๊อนนน\n",
      "ขี้เกียด -> ขี้เกียจ\n",
      "น้าาาาา -> น้าาาาา\n",
      "ไฟแนน -> คะแนน\n",
      "ใด้ -> ใด้\n",
      "ใหน -> ใหน\n",
      "รุ้ -> รู้\n",
      "ค้าบบบบ -> ค้าบบบบ\n",
      "น้าา -> น้าา\n",
      "จ้า -> จ้า\n",
      "หน้าเอ็นดู -> หน้าเอ็นดู\n",
      "รูกกกกกกก -> กกกกกกกก\n",
      "มะนุด -> สะดุด\n",
      "ยุ่ -> ยุ่\n",
      "ขอบคุน -> ขอบคุณ\n",
      "นะค่ะ -> ค่ะ\n",
      "เจ้างพาภ -> เจ้างพาภ\n",
      "ฝั่ง -> ฝั่ง\n",
      "สายพัน -> สารพัน\n",
      "เลยยย -> เลยยย\n",
      "มากกก -> มาก\n",
      "มะวาน -> หวาน\n",
      "เอ๊ยยยยยย -> เจ๊ยยยยย\n",
      "คือออออออ -> ออออออออ\n",
      "เล้ยยยยยยยยย -> เล้ยยยยยยยยย\n",
      "มากกกกกกกก -> กกกกกกกก\n",
      "จ้าาาาา -> จ้าาาาา\n",
      "ซิ -> ซิ\n",
      "คับ -> คับ\n",
      "อนุญาติ -> อนุญาต\n",
      "ครัฟ -> ครับ\n",
      "แกรน -> แกรน\n",
      "มังสาวิรัติ -> มังสวิรัติ\n",
      "แข๊งแรง -> แข็งแรง\n"
     ]
    }
   ],
   "source": [
    "from pythainlp.spell import correct\n",
    "def read_master_file(file_path,delimeter=None):\n",
    "    raw_str = ''\n",
    "    master_list = []\n",
    "    for fn in glob.glob(file_path):\n",
    "        with open (fn) as f:\n",
    "            original = f.readlines()\n",
    "            lines = [line.rstrip('\\n') for line in original]\n",
    "            for each_line in lines: \n",
    "                raw_str = raw_str + each_line\n",
    "            if delimeter is not None:\n",
    "                master_list.extend(raw_str.split(delimeter))\n",
    "            else:\n",
    "                master_list.extend(lines)\n",
    "                \n",
    "    return master_list\n",
    "\n",
    "master_slangs = read_master_file('slangs_corrected', None) \n",
    "print(master_slangs)\n",
    "\n",
    "\n",
    "\n",
    "slangs = [\n",
    "'โล๊ะ','น่าจ้า',\n",
    "'น่า','จ้าาา',\n",
    "'เปน','โรงบาล',\n",
    "'จิง','พักก๊อนนน',\n",
    "'ขี้เกียด','น้าาาาา',\n",
    "'ไฟแนน','ใด้',\n",
    "'ใหน','รุ้','ค้าบบบบ',\n",
    " 'น้าา',\n",
    "'จ้า','หน้าเอ็นดู',\n",
    "'รูกกกกกกก','มะนุด',\n",
    "'ยุ่','ขอบคุน',\n",
    "'นะค่ะ','เจ้างพาภ',\n",
    "'ฝั่ง','สายพัน',\n",
    "'เลยยย','มากกก',\n",
    "'มะวาน','เอ๊ยยยยยย',\n",
    "'คือออออออ','เล้ยยยยยยยยย',\n",
    "'มากกกกกกกก','จ้าาาาา',\n",
    "'ซิ','คับ',\n",
    "'อนุญาติ','ครัฟ',\n",
    "'แกรน','มังสาวิรัติ',\n",
    "'แข๊งแรง'\n",
    "]\n",
    "# print(\"Slangs\" + '\\t\\t\\t'+  \"Correct Spelling\"+ '\\t\\t\\t'+ 'PythaiNLP')\n",
    "# # slangs_enu = enumerate(bb, 1)\n",
    "# # print(slangs_enu)\n",
    "# # correct_enu = enumerate(lines, 1)\n",
    "\n",
    "#print ('Slangs'+ '\\t' + 'System output')\n",
    "for i in slangs: \n",
    "    print (\"{} -> {}\".format(i, correct(i)))\n",
    "# #     print ('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hyp โล๊ะ System โล๊ะ Ref โละ\n",
      "[['โ'], ['ล'], ['๊'], ['ะ']]\n",
      "[['โ'], ['ล'], ['๊'], ['ะ']]\n",
      "[['โ'], ['ล'], ['ะ']]\n",
      "\n",
      "Hyp น่าจ้า System ค่าจ้าง Ref นะจ๊ะ\n",
      "[['น'], ['่'], ['า'], ['จ'], ['้'], ['า']]\n",
      "[['ค'], ['่'], ['า'], ['จ'], ['้'], ['า'], ['ง']]\n",
      "[['น'], ['ะ'], ['จ'], ['๊'], ['ะ']]\n",
      "\n",
      "Hyp น่า System น่า Ref นะ\n",
      "[['น'], ['่'], ['า']]\n",
      "[['น'], ['่'], ['า']]\n",
      "[['น'], ['ะ']]\n",
      "\n",
      "Hyp จ้าาา System จ้าาา Ref จ๊ะ\n",
      "[['จ'], ['้'], ['า'], ['า'], ['า']]\n",
      "[['จ'], ['้'], ['า'], ['า'], ['า']]\n",
      "[['จ'], ['๊'], ['ะ']]\n",
      "\n",
      "Hyp เปน System เปน Ref เป็น\n",
      "[['เ'], ['ป'], ['น']]\n",
      "[['เ'], ['ป'], ['น']]\n",
      "[['เ'], ['ป'], ['็'], ['น']]\n",
      "\n",
      "Hyp โรงบาล System โรงงาน Ref โรงพยาบาล\n",
      "[['โ'], ['ร'], ['ง'], ['บ'], ['า'], ['ล']]\n",
      "[['โ'], ['ร'], ['ง'], ['ง'], ['า'], ['น']]\n",
      "[['โ'], ['ร'], ['ง'], ['พ'], ['ย'], ['า'], ['บ'], ['า'], ['ล']]\n",
      "\n",
      "Hyp จิง System จิง Ref จริง\n",
      "[['จ'], ['ิ'], ['ง']]\n",
      "[['จ'], ['ิ'], ['ง']]\n",
      "[['จ'], ['ร'], ['ิ'], ['ง']]\n",
      "\n",
      "Hyp พักก๊อนนน System พักก๊อนนน Ref พักก่อน\n",
      "[['พ'], ['ั'], ['ก'], ['ก'], ['๊'], ['อ'], ['น'], ['น'], ['น']]\n",
      "[['พ'], ['ั'], ['ก'], ['ก'], ['๊'], ['อ'], ['น'], ['น'], ['น']]\n",
      "[['พ'], ['ั'], ['ก'], ['ก'], ['่'], ['อ'], ['น']]\n",
      "\n",
      "Hyp ขี้เกียด System ขี้เกียจ Ref ขี้เกียจ\n",
      "[['ข'], ['ี'], ['้'], ['เ'], ['ก'], ['ี'], ['ย'], ['ด']]\n",
      "[['ข'], ['ี'], ['้'], ['เ'], ['ก'], ['ี'], ['ย'], ['จ']]\n",
      "[['ข'], ['ี'], ['้'], ['เ'], ['ก'], ['ี'], ['ย'], ['จ']]\n",
      "\n",
      "Hyp น้าาาาา System น้าาาาา Ref นะ\n",
      "[['น'], ['้'], ['า'], ['า'], ['า'], ['า'], ['า']]\n",
      "[['น'], ['้'], ['า'], ['า'], ['า'], ['า'], ['า']]\n",
      "[['น'], ['ะ']]\n",
      "\n",
      "Hyp ไฟแนน System คะแนน Ref ไฟแนนซ์\n",
      "[['ไ'], ['ฟ'], ['แ'], ['น'], ['น']]\n",
      "[['ค'], ['ะ'], ['แ'], ['น'], ['น']]\n",
      "[['ไ'], ['ฟ'], ['แ'], ['น'], ['น'], ['ซ'], ['์']]\n",
      "\n",
      "Hyp ใด้ System ใด้ Ref ได้\n",
      "[['ใ'], ['ด'], ['้']]\n",
      "[['ใ'], ['ด'], ['้']]\n",
      "[['ไ'], ['ด'], ['้']]\n",
      "\n",
      "Hyp ใหน System ใหน Ref ไหน\n",
      "[['ใ'], ['ห'], ['น']]\n",
      "[['ใ'], ['ห'], ['น']]\n",
      "[['ไ'], ['ห'], ['น']]\n",
      "\n",
      "Hyp รุ้ System รู้ Ref รู้\n",
      "[['ร'], ['ุ'], ['้']]\n",
      "[['ร'], ['ู'], ['้']]\n",
      "[['ร'], ['ู'], ['้']]\n",
      "\n",
      "Hyp ค้าบบบบ System ค้าบบบบ Ref ครับ\n",
      "[['ค'], ['้'], ['า'], ['บ'], ['บ'], ['บ'], ['บ']]\n",
      "[['ค'], ['้'], ['า'], ['บ'], ['บ'], ['บ'], ['บ']]\n",
      "[['ค'], ['ร'], ['ั'], ['บ']]\n",
      "\n",
      "Hyp น้าา System น้าา Ref นะ\n",
      "[['น'], ['้'], ['า'], ['า']]\n",
      "[['น'], ['้'], ['า'], ['า']]\n",
      "[['น'], ['ะ']]\n",
      "\n",
      "Hyp จ้า System จ้า Ref จ๊ะ\n",
      "[['จ'], ['้'], ['า']]\n",
      "[['จ'], ['้'], ['า']]\n",
      "[['จ'], ['๊'], ['ะ']]\n",
      "\n",
      "Hyp หน้าเอ็นดู System หน้าเอ็นดู Ref น่าเอ็นดู\n",
      "[['ห'], ['น'], ['้'], ['า'], ['เ'], ['อ'], ['็'], ['น'], ['ด'], ['ู']]\n",
      "[['ห'], ['น'], ['้'], ['า'], ['เ'], ['อ'], ['็'], ['น'], ['ด'], ['ู']]\n",
      "[['น'], ['่'], ['า'], ['เ'], ['อ'], ['็'], ['น'], ['ด'], ['ู']]\n",
      "\n",
      "Hyp รูกกกกกกก System กกกกกกกก Ref ลูก\n",
      "[['ร'], ['ู'], ['ก'], ['ก'], ['ก'], ['ก'], ['ก'], ['ก'], ['ก']]\n",
      "[['ก'], ['ก'], ['ก'], ['ก'], ['ก'], ['ก'], ['ก'], ['ก']]\n",
      "[['ล'], ['ู'], ['ก']]\n",
      "\n",
      "Hyp มะนุด System สะดุด Ref มนุษย์\n",
      "[['ม'], ['ะ'], ['น'], ['ุ'], ['ด']]\n",
      "[['ส'], ['ะ'], ['ด'], ['ุ'], ['ด']]\n",
      "[['ม'], ['น'], ['ุ'], ['ษ'], ['ย'], ['์']]\n",
      "\n",
      "Hyp ยุ่ System ยุ่ Ref อยู่\n",
      "[['ย'], ['ุ'], ['่']]\n",
      "[['ย'], ['ุ'], ['่']]\n",
      "[['อ'], ['ย'], ['ู'], ['่']]\n",
      "\n",
      "Hyp ขอบคุน System ขอบคุณ Ref ขอบคุณ\n",
      "[['ข'], ['อ'], ['บ'], ['ค'], ['ุ'], ['น']]\n",
      "[['ข'], ['อ'], ['บ'], ['ค'], ['ุ'], ['ณ']]\n",
      "[['ข'], ['อ'], ['บ'], ['ค'], ['ุ'], ['ณ']]\n",
      "\n",
      "Hyp นะค่ะ System ค่ะ Ref นะคะ\n",
      "[['น'], ['ะ'], ['ค'], ['่'], ['ะ']]\n",
      "[['ค'], ['่'], ['ะ']]\n",
      "[['น'], ['ะ'], ['ค'], ['ะ']]\n",
      "\n",
      "Hyp เจ้างพาภ System เจ้างพาภ Ref เจ้าภาพ\n",
      "[['เ'], ['จ'], ['้'], ['า'], ['ง'], ['พ'], ['า'], ['ภ']]\n",
      "[['เ'], ['จ'], ['้'], ['า'], ['ง'], ['พ'], ['า'], ['ภ']]\n",
      "[['เ'], ['จ'], ['้'], ['า'], ['ภ'], ['า'], ['พ']]\n",
      "\n",
      "Hyp ฝั่ง System ฝั่ง Ref ฝัง\n",
      "[['ฝ'], ['ั'], ['่'], ['ง']]\n",
      "[['ฝ'], ['ั'], ['่'], ['ง']]\n",
      "[['ฝ'], ['ั'], ['ง']]\n",
      "\n",
      "Hyp สายพัน System สารพัน Ref สายพันธุ์\n",
      "[['ส'], ['า'], ['ย'], ['พ'], ['ั'], ['น']]\n",
      "[['ส'], ['า'], ['ร'], ['พ'], ['ั'], ['น']]\n",
      "[['ส'], ['า'], ['ย'], ['พ'], ['ั'], ['น'], ['ธ'], ['ุ'], ['์']]\n",
      "\n",
      "Hyp เลยยย System เลยยย Ref เลย\n",
      "[['เ'], ['ล'], ['ย'], ['ย'], ['ย']]\n",
      "[['เ'], ['ล'], ['ย'], ['ย'], ['ย']]\n",
      "[['เ'], ['ล'], ['ย']]\n",
      "\n",
      "Hyp มากกก System มาก Ref มาก\n",
      "[['ม'], ['า'], ['ก'], ['ก'], ['ก']]\n",
      "[['ม'], ['า'], ['ก']]\n",
      "[['ม'], ['า'], ['ก']]\n",
      "\n",
      "Hyp มะวาน System หวาน Ref เมื่อวาน\n",
      "[['ม'], ['ะ'], ['ว'], ['า'], ['น']]\n",
      "[['ห'], ['ว'], ['า'], ['น']]\n",
      "[['เ'], ['ม'], ['ื'], ['่'], ['อ'], ['ว'], ['า'], ['น']]\n",
      "\n",
      "Hyp เอ๊ยยยยยย System เจ๊ยยยยย Ref เอ๊ย\n",
      "[['เ'], ['อ'], ['๊'], ['ย'], ['ย'], ['ย'], ['ย'], ['ย'], ['ย']]\n",
      "[['เ'], ['จ'], ['๊'], ['ย'], ['ย'], ['ย'], ['ย'], ['ย']]\n",
      "[['เ'], ['อ'], ['๊'], ['ย']]\n",
      "\n",
      "Hyp คือออออออ System ออออออออ Ref คือ\n",
      "[['ค'], ['ื'], ['อ'], ['อ'], ['อ'], ['อ'], ['อ'], ['อ'], ['อ']]\n",
      "[['อ'], ['อ'], ['อ'], ['อ'], ['อ'], ['อ'], ['อ'], ['อ']]\n",
      "[['ค'], ['ื'], ['อ']]\n",
      "\n",
      "Hyp เล้ยยยยยยยยย System เล้ยยยยยยยยย Ref เลย\n",
      "[['เ'], ['ล'], ['้'], ['ย'], ['ย'], ['ย'], ['ย'], ['ย'], ['ย'], ['ย'], ['ย'], ['ย']]\n",
      "[['เ'], ['ล'], ['้'], ['ย'], ['ย'], ['ย'], ['ย'], ['ย'], ['ย'], ['ย'], ['ย'], ['ย']]\n",
      "[['เ'], ['ล'], ['ย']]\n",
      "\n",
      "Hyp มากกกกกกกก System กกกกกกกก Ref มาก\n",
      "[['ม'], ['า'], ['ก'], ['ก'], ['ก'], ['ก'], ['ก'], ['ก'], ['ก'], ['ก']]\n",
      "[['ก'], ['ก'], ['ก'], ['ก'], ['ก'], ['ก'], ['ก'], ['ก']]\n",
      "[['ม'], ['า'], ['ก']]\n",
      "\n",
      "Hyp จ้าาาาา System จ้าาาาา Ref จ๊ะ\n",
      "[['จ'], ['้'], ['า'], ['า'], ['า'], ['า'], ['า']]\n",
      "[['จ'], ['้'], ['า'], ['า'], ['า'], ['า'], ['า']]\n",
      "[['จ'], ['๊'], ['ะ']]\n",
      "\n",
      "Hyp ซิ System ซิ Ref สิ\n",
      "[['ซ'], ['ิ']]\n",
      "[['ซ'], ['ิ']]\n",
      "[['ส'], ['ิ']]\n",
      "\n",
      "Hyp คับ System คับ Ref ครับ\n",
      "[['ค'], ['ั'], ['บ']]\n",
      "[['ค'], ['ั'], ['บ']]\n",
      "[['ค'], ['ร'], ['ั'], ['บ']]\n",
      "\n",
      "Hyp อนุญาติ System อนุญาต Ref อนุญาต\n",
      "[['อ'], ['น'], ['ุ'], ['ญ'], ['า'], ['ต'], ['ิ']]\n",
      "[['อ'], ['น'], ['ุ'], ['ญ'], ['า'], ['ต']]\n",
      "[['อ'], ['น'], ['ุ'], ['ญ'], ['า'], ['ต']]\n",
      "\n",
      "Hyp ครัฟ System ครับ Ref ครับ\n",
      "[['ค'], ['ร'], ['ั'], ['ฟ']]\n",
      "[['ค'], ['ร'], ['ั'], ['บ']]\n",
      "[['ค'], ['ร'], ['ั'], ['บ']]\n",
      "\n",
      "Hyp แกรน System แกรน Ref แกร็น\n",
      "[['แ'], ['ก'], ['ร'], ['น']]\n",
      "[['แ'], ['ก'], ['ร'], ['น']]\n",
      "[['แ'], ['ก'], ['ร'], ['็'], ['น']]\n",
      "\n",
      "Hyp มังสาวิรัติ System มังสวิรัติ Ref มังสวิรัติ\n",
      "[['ม'], ['ั'], ['ง'], ['ส'], ['า'], ['ว'], ['ิ'], ['ร'], ['ั'], ['ต'], ['ิ']]\n",
      "[['ม'], ['ั'], ['ง'], ['ส'], ['ว'], ['ิ'], ['ร'], ['ั'], ['ต'], ['ิ']]\n",
      "[['ม'], ['ั'], ['ง'], ['ส'], ['ว'], ['ิ'], ['ร'], ['ั'], ['ต'], ['ิ']]\n",
      "\n",
      "Hyp แข๊งแรง System แข็งแรง Ref แข็งแรง\n",
      "[['แ'], ['ข'], ['๊'], ['ง'], ['แ'], ['ร'], ['ง']]\n",
      "[['แ'], ['ข'], ['็'], ['ง'], ['แ'], ['ร'], ['ง']]\n",
      "[['แ'], ['ข'], ['็'], ['ง'], ['แ'], ['ร'], ['ง']]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def align(x, y):\n",
    "    x = ['#'] + x; y = ['#'] + y\n",
    "    m = len(x); n = len(y)\n",
    "    C = [[[0, None]] * n for i in range(m)]\n",
    "    for j in range(1, n-1): C[0][j] = [C[0][j-1][0]+1, (0, j-1)]\n",
    "    for i in range(1, m-1): C[i][0] = [C[i-1][0][0]+1, (i-1, 0)]\n",
    "    for i in range(1, m):\n",
    "        for j in range(1, n):\n",
    "            C[i][j] = min([[C[i-1][j-1][0]+int(x[i]!=y[j]), (i-1, j-1)],\n",
    "                           [C[i-1][j][0]+1, (i-1, j)],\n",
    "                           [C[i][j-1][0]+1, (i, j-1)]])\n",
    "    crumbs = [(m-1, n-1)]\n",
    "    while crumbs[-1] != (0, 0):\n",
    "        i, j = crumbs[-1]\n",
    "        crumbs.append(C[i][j][1])\n",
    "    crumbs.reverse()\n",
    "    xa = []; ya = []\n",
    "    for t in range(1, len(crumbs)):\n",
    "        pi, pj = crumbs[t-1]\n",
    "        i, j = crumbs[t]\n",
    "        if pi == i: xa.append('__')\n",
    "        elif pi != i: xa.append(x[i])\n",
    "        if pj == j: ya.append('__')\n",
    "        elif pj != j: ya.append(y[j])\n",
    "    return xa, ya\n",
    "\n",
    "slangs = [\n",
    "'โล๊ะ','น่าจ้า',\n",
    "'น่า','จ้าาา',\n",
    "'เปน','โรงบาล',\n",
    "'จิง','พักก๊อนนน',\n",
    "'ขี้เกียด','น้าาาาา',\n",
    "'ไฟแนน','ใด้',\n",
    "'ใหน','รุ้','ค้าบบบบ',\n",
    " 'น้าา',\n",
    "'จ้า','หน้าเอ็นดู',\n",
    "'รูกกกกกกก','มะนุด',\n",
    "'ยุ่','ขอบคุน',\n",
    "'นะค่ะ','เจ้างพาภ',\n",
    "'ฝั่ง','สายพัน',\n",
    "'เลยยย','มากกก',\n",
    "'มะวาน','เอ๊ยยยยยย',\n",
    "'คือออออออ','เล้ยยยยยยยยย',\n",
    "'มากกกกกกกก','จ้าาาาา',\n",
    "'ซิ','คับ',\n",
    "'อนุญาติ','ครัฟ',\n",
    "'แกรน','มังสาวิรัติ',\n",
    "'แข๊งแรง'\n",
    "]\n",
    "\n",
    "with open ('slangs_corrected') as f:\n",
    "    p = f.readlines()\n",
    "    master = [o.rstrip('\\n') for o in p]\n",
    "    \n",
    "# hyp = 'ขี้เกียด'\n",
    "# ref = 'ขี้เกียจ'\n",
    "# x = [i.split() for i in hyp]\n",
    "# y = [b.split() for b in ref]\n",
    "\n",
    "# ha, ra = align(x,y)\n",
    "\n",
    "for i in range(len(slangs)): \n",
    "   \n",
    "    corrected = correct(slangs[i])\n",
    "    print('Hyp',slangs[i], 'System', corrected, 'Ref', master[i])\n",
    "    x = [a.split() for a in slangs[i]]\n",
    "    y = [b.split() for b in corrected]\n",
    "    z = [c.split() for c in master[i]]\n",
    "    print(x)\n",
    "    print(y)\n",
    "    print(z)\n",
    "    print('')\n",
    "    \n",
    "hyp, ms = align(x,z)\n",
    "hyp, rf = align(x,y)    \n",
    "#     print (\"{} -> {}\".format(i, correct(i)))\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n"
     ]
    }
   ],
   "source": [
    "a = [['โ'], ['ล'], '__', ['ะ']]\n",
    "\n",
    "def count_char(l):\n",
    "    count = 0\n",
    "    for c in a:\n",
    "        if c != '__':\n",
    "            count += 1\n",
    "    return count\n",
    "\n",
    "print(count_char(a))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C 0 [[0, None], [1, (0, 0)], [2, (0, 1)], [3, (0, 2)], [4, (0, 3)], [0, None]]\n",
      "C 1 [[1, (0, 0)], [0, (0, 0)], [1, (1, 1)], [2, (1, 2)], [3, (1, 3)], [1, (0, 5)]]\n",
      "C 2 [[2, (1, 0)], [1, (1, 1)], [1, (1, 1)], [2, (1, 2)], [3, (1, 3)], [2, (1, 5)]]\n",
      "C 3 [[3, (2, 0)], [2, (2, 1)], [2, (2, 1)], [2, (2, 2)], [3, (2, 3)], [3, (2, 5)]]\n",
      "C 4 [[4, (3, 0)], [3, (3, 1)], [3, (3, 1)], [2, (3, 2)], [3, (3, 3)], [4, (3, 4)]]\n",
      "C 5 [[5, (4, 0)], [4, (4, 1)], [4, (4, 1)], [3, (4, 3)], [3, (4, 3)], [4, (4, 4)]]\n",
      "C 6 [[0, None], [1, (6, 0)], [2, (6, 1)], [3, (6, 2)], [4, (5, 3)], [4, (5, 4)]]\n",
      "\n",
      "(6, 5)\n",
      "C: (5, 4)\n",
      "crumbs: [(6, 5), (5, 4)]\n",
      "(5, 4)\n",
      "C: (4, 3)\n",
      "crumbs: [(6, 5), (5, 4), (4, 3)]\n",
      "(4, 3)\n",
      "C: (3, 2)\n",
      "crumbs: [(6, 5), (5, 4), (4, 3), (3, 2)]\n",
      "(3, 2)\n",
      "C: (2, 1)\n",
      "crumbs: [(6, 5), (5, 4), (4, 3), (3, 2), (2, 1)]\n",
      "(2, 1)\n",
      "C: (1, 1)\n",
      "crumbs: [(6, 5), (5, 4), (4, 3), (3, 2), (2, 1), (1, 1)]\n",
      "(1, 1)\n",
      "C: (0, 0)\n",
      "crumbs: [(6, 5), (5, 4), (4, 3), (3, 2), (2, 1), (1, 1), (0, 0)]\n"
     ]
    }
   ],
   "source": [
    "x = [['น'], ['่'], ['า'], ['จ'], ['้'], ['า']]\n",
    "# x = [['ค'], ['่'], ['า'], ['จ'], ['้'], ['า'], ['ง']]\n",
    "y = [['น'], ['ะ'], ['จ'], ['๊'], ['ะ']]\n",
    "# x = [['โ'], ['ล'], ['๊'], ['ะ']]\n",
    "# y = [['โ'], ['ล'], ['ะ']]\n",
    "# x = [['จ'], ['้'], ['า'], ['า'], ['า']]\n",
    "# y = [['จ'], ['๊'], ['ะ']]\n",
    "\n",
    "x = ['#'] + x; y = ['#'] + y\n",
    "m = len(x); n = len(y)\n",
    "C = [[[0, None]] * n for i in range(m)]\n",
    "for j in range(1, n-1): C[0][j] = [C[0][j-1][0]+1, (0, j-1)]\n",
    "for i in range(1, m-1): C[i][0] = [C[i-1][0][0]+1, (i-1, 0)]\n",
    "for i in range(1, m):\n",
    "    for j in range(1, n):\n",
    "        C[i][j] = min([[C[i-1][j-1][0]+int(x[i]!=y[j]), (i-1, j-1)],\n",
    "                       [C[i-1][j][0]+1, (i-1, j)],\n",
    "                       [C[i][j-1][0]+1, (i, j-1)]])\n",
    "\n",
    "# print(C, len(C))\n",
    "for i in range(len(C)):\n",
    "    print('C',i, C[i])\n",
    "print('')\n",
    "crumbs = [(m-1, n-1)]\n",
    "# print(crumbs[-1])\n",
    "while crumbs[-1] != (0, 0):\n",
    "    print(crumbs[-1])\n",
    "    i, j = crumbs[-1]\n",
    "    print('C:',C[i][j][1])\n",
    "    crumbs.append(C[i][j][1])\n",
    "    print('crumbs:',crumbs)\n",
    "crumbs.reverse()\n",
    "xa = []; ya = []\n",
    "for t in range(1, len(crumbs)):\n",
    "    pi, pj = crumbs[t-1]\n",
    "    i, j = crumbs[t]\n",
    "    if pi == i: xa.append('__')\n",
    "    elif pi != i: xa.append(x[i])\n",
    "    if pj == j: ya.append('__')\n",
    "    elif pj != j: ya.append(y[j])\n",
    "return xa, ya\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5 3\n"
     ]
    }
   ],
   "source": [
    "c = (5,3)\n",
    "i,j = c\n",
    "print(i,j)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hyp โล๊ะ System โล๊ะ Ref โละ\n",
      "[['โ'], ['ล'], ['๊'], ['ะ']]\n",
      "[['โ'], ['ล'], ['๊'], ['ะ']]\n",
      "[['โ'], ['ล'], ['ะ']]\n",
      "\n",
      "HYP1 aligned:  [['โ'], ['ล'], ['๊'], ['ะ']]\n",
      "ms aligned:  [['โ'], ['ล'], '__', ['ะ']]\n",
      "HYP2 aligned:  [['โ'], ['ล'], ['๊'], ['ะ']]\n",
      "ms aligned:  [['โ'], ['ล'], '__', ['ะ']]\n",
      "Hyp น่าจ้า System ค่าจ้าง Ref นะจ๊ะ\n",
      "[['น'], ['่'], ['า'], ['จ'], ['้'], ['า']]\n",
      "[['ค'], ['่'], ['า'], ['จ'], ['้'], ['า'], ['ง']]\n",
      "[['น'], ['ะ'], ['จ'], ['๊'], ['ะ']]\n",
      "\n",
      "HYP1 aligned:  [['น'], ['่'], ['า'], ['จ'], ['้'], ['า']]\n",
      "ms aligned:  [['น'], '__', ['ะ'], ['จ'], ['๊'], ['ะ']]\n",
      "HYP2 aligned:  [['ค'], ['่'], ['า'], ['จ'], ['้'], ['า'], ['ง']]\n",
      "ms aligned:  ['__', ['น'], ['ะ'], ['จ'], '__', ['๊'], ['ะ']]\n",
      "Hyp น่า System น่า Ref นะ\n",
      "[['น'], ['่'], ['า']]\n",
      "[['น'], ['่'], ['า']]\n",
      "[['น'], ['ะ']]\n",
      "\n",
      "HYP1 aligned:  [['น'], ['่'], ['า']]\n",
      "ms aligned:  [['น'], '__', ['ะ']]\n",
      "HYP2 aligned:  [['น'], ['่'], ['า']]\n",
      "ms aligned:  [['น'], '__', ['ะ']]\n",
      "Hyp จ้าาา System จ้าาา Ref จ๊ะ\n",
      "[['จ'], ['้'], ['า'], ['า'], ['า']]\n",
      "[['จ'], ['้'], ['า'], ['า'], ['า']]\n",
      "[['จ'], ['๊'], ['ะ']]\n",
      "\n",
      "HYP1 aligned:  [['จ'], ['้'], ['า'], ['า'], ['า']]\n",
      "ms aligned:  [['จ'], '__', '__', ['๊'], ['ะ']]\n",
      "HYP2 aligned:  [['จ'], ['้'], ['า'], ['า'], ['า']]\n",
      "ms aligned:  [['จ'], '__', '__', ['๊'], ['ะ']]\n",
      "Hyp เปน System เปน Ref เป็น\n",
      "[['เ'], ['ป'], ['น']]\n",
      "[['เ'], ['ป'], ['น']]\n",
      "[['เ'], ['ป'], ['็'], ['น']]\n",
      "\n",
      "HYP1 aligned:  [['เ'], ['ป'], '__', ['น']]\n",
      "ms aligned:  [['เ'], ['ป'], ['็'], ['น']]\n",
      "HYP2 aligned:  [['เ'], ['ป'], '__', ['น']]\n",
      "ms aligned:  [['เ'], ['ป'], ['็'], ['น']]\n",
      "Hyp โรงบาล System โรงงาน Ref โรงพยาบาล\n",
      "[['โ'], ['ร'], ['ง'], ['บ'], ['า'], ['ล']]\n",
      "[['โ'], ['ร'], ['ง'], ['ง'], ['า'], ['น']]\n",
      "[['โ'], ['ร'], ['ง'], ['พ'], ['ย'], ['า'], ['บ'], ['า'], ['ล']]\n",
      "\n",
      "HYP1 aligned:  [['โ'], ['ร'], ['ง'], '__', '__', '__', ['บ'], ['า'], ['ล']]\n",
      "ms aligned:  [['โ'], ['ร'], ['ง'], ['พ'], ['ย'], ['า'], ['บ'], ['า'], ['ล']]\n",
      "HYP2 aligned:  [['โ'], ['ร'], ['ง'], '__', '__', '__', ['ง'], ['า'], ['น']]\n",
      "ms aligned:  [['โ'], ['ร'], ['ง'], ['พ'], ['ย'], ['า'], ['บ'], ['า'], ['ล']]\n",
      "Hyp จิง System จิง Ref จริง\n",
      "[['จ'], ['ิ'], ['ง']]\n",
      "[['จ'], ['ิ'], ['ง']]\n",
      "[['จ'], ['ร'], ['ิ'], ['ง']]\n",
      "\n",
      "HYP1 aligned:  [['จ'], '__', ['ิ'], ['ง']]\n",
      "ms aligned:  [['จ'], ['ร'], ['ิ'], ['ง']]\n",
      "HYP2 aligned:  [['จ'], '__', ['ิ'], ['ง']]\n",
      "ms aligned:  [['จ'], ['ร'], ['ิ'], ['ง']]\n",
      "Hyp พักก๊อนนน System พักก๊อนนน Ref พักก่อน\n",
      "[['พ'], ['ั'], ['ก'], ['ก'], ['๊'], ['อ'], ['น'], ['น'], ['น']]\n",
      "[['พ'], ['ั'], ['ก'], ['ก'], ['๊'], ['อ'], ['น'], ['น'], ['น']]\n",
      "[['พ'], ['ั'], ['ก'], ['ก'], ['่'], ['อ'], ['น']]\n",
      "\n",
      "HYP1 aligned:  [['พ'], ['ั'], ['ก'], ['ก'], ['๊'], ['อ'], ['น'], ['น'], ['น']]\n",
      "ms aligned:  [['พ'], ['ั'], ['ก'], ['ก'], ['่'], ['อ'], '__', '__', ['น']]\n",
      "HYP2 aligned:  [['พ'], ['ั'], ['ก'], ['ก'], ['๊'], ['อ'], ['น'], ['น'], ['น']]\n",
      "ms aligned:  [['พ'], ['ั'], ['ก'], ['ก'], ['่'], ['อ'], '__', '__', ['น']]\n",
      "Hyp ขี้เกียด System ขี้เกียจ Ref ขี้เกียจ\n",
      "[['ข'], ['ี'], ['้'], ['เ'], ['ก'], ['ี'], ['ย'], ['ด']]\n",
      "[['ข'], ['ี'], ['้'], ['เ'], ['ก'], ['ี'], ['ย'], ['จ']]\n",
      "[['ข'], ['ี'], ['้'], ['เ'], ['ก'], ['ี'], ['ย'], ['จ']]\n",
      "\n",
      "HYP1 aligned:  [['ข'], ['ี'], ['้'], ['เ'], ['ก'], ['ี'], ['ย'], ['ด']]\n",
      "ms aligned:  [['ข'], ['ี'], ['้'], ['เ'], ['ก'], ['ี'], ['ย'], ['จ']]\n",
      "HYP2 aligned:  [['ข'], ['ี'], ['้'], ['เ'], ['ก'], ['ี'], ['ย'], ['จ']]\n",
      "ms aligned:  [['ข'], ['ี'], ['้'], ['เ'], ['ก'], ['ี'], ['ย'], ['จ']]\n",
      "Hyp น้าาาาา System น้าาาาา Ref นะ\n",
      "[['น'], ['้'], ['า'], ['า'], ['า'], ['า'], ['า']]\n",
      "[['น'], ['้'], ['า'], ['า'], ['า'], ['า'], ['า']]\n",
      "[['น'], ['ะ']]\n",
      "\n",
      "HYP1 aligned:  [['น'], ['้'], ['า'], ['า'], ['า'], ['า'], ['า']]\n",
      "ms aligned:  [['น'], '__', '__', '__', '__', '__', ['ะ']]\n",
      "HYP2 aligned:  [['น'], ['้'], ['า'], ['า'], ['า'], ['า'], ['า']]\n",
      "ms aligned:  [['น'], '__', '__', '__', '__', '__', ['ะ']]\n",
      "Hyp ไฟแนน System คะแนน Ref ไฟแนนซ์\n",
      "[['ไ'], ['ฟ'], ['แ'], ['น'], ['น']]\n",
      "[['ค'], ['ะ'], ['แ'], ['น'], ['น']]\n",
      "[['ไ'], ['ฟ'], ['แ'], ['น'], ['น'], ['ซ'], ['์']]\n",
      "\n",
      "HYP1 aligned:  [['ไ'], ['ฟ'], ['แ'], ['น'], ['น'], '__', '__']\n",
      "ms aligned:  [['ไ'], ['ฟ'], ['แ'], ['น'], ['น'], ['ซ'], ['์']]\n",
      "HYP2 aligned:  [['ค'], ['ะ'], ['แ'], ['น'], ['น'], '__', '__']\n",
      "ms aligned:  [['ไ'], ['ฟ'], ['แ'], ['น'], ['น'], ['ซ'], ['์']]\n",
      "Hyp ใด้ System ใด้ Ref ได้\n",
      "[['ใ'], ['ด'], ['้']]\n",
      "[['ใ'], ['ด'], ['้']]\n",
      "[['ไ'], ['ด'], ['้']]\n",
      "\n",
      "HYP1 aligned:  [['ใ'], ['ด'], ['้']]\n",
      "ms aligned:  [['ไ'], ['ด'], ['้']]\n",
      "HYP2 aligned:  [['ใ'], ['ด'], ['้']]\n",
      "ms aligned:  [['ไ'], ['ด'], ['้']]\n",
      "Hyp ใหน System ใหน Ref ไหน\n",
      "[['ใ'], ['ห'], ['น']]\n",
      "[['ใ'], ['ห'], ['น']]\n",
      "[['ไ'], ['ห'], ['น']]\n",
      "\n",
      "HYP1 aligned:  [['ใ'], ['ห'], ['น']]\n",
      "ms aligned:  [['ไ'], ['ห'], ['น']]\n",
      "HYP2 aligned:  [['ใ'], ['ห'], ['น']]\n",
      "ms aligned:  [['ไ'], ['ห'], ['น']]\n",
      "Hyp รุ้ System รู้ Ref รู้\n",
      "[['ร'], ['ุ'], ['้']]\n",
      "[['ร'], ['ู'], ['้']]\n",
      "[['ร'], ['ู'], ['้']]\n",
      "\n",
      "HYP1 aligned:  [['ร'], ['ุ'], ['้']]\n",
      "ms aligned:  [['ร'], ['ู'], ['้']]\n",
      "HYP2 aligned:  [['ร'], ['ู'], ['้']]\n",
      "ms aligned:  [['ร'], ['ู'], ['้']]\n",
      "Hyp ค้าบบบบ System ค้าบบบบ Ref ครับ\n",
      "[['ค'], ['้'], ['า'], ['บ'], ['บ'], ['บ'], ['บ']]\n",
      "[['ค'], ['้'], ['า'], ['บ'], ['บ'], ['บ'], ['บ']]\n",
      "[['ค'], ['ร'], ['ั'], ['บ']]\n",
      "\n",
      "HYP1 aligned:  [['ค'], ['้'], ['า'], ['บ'], ['บ'], ['บ'], ['บ']]\n",
      "ms aligned:  [['ค'], '__', '__', '__', ['ร'], ['ั'], ['บ']]\n",
      "HYP2 aligned:  [['ค'], ['้'], ['า'], ['บ'], ['บ'], ['บ'], ['บ']]\n",
      "ms aligned:  [['ค'], '__', '__', '__', ['ร'], ['ั'], ['บ']]\n",
      "Hyp น้าา System น้าา Ref นะ\n",
      "[['น'], ['้'], ['า'], ['า']]\n",
      "[['น'], ['้'], ['า'], ['า']]\n",
      "[['น'], ['ะ']]\n",
      "\n",
      "HYP1 aligned:  [['น'], ['้'], ['า'], ['า']]\n",
      "ms aligned:  [['น'], '__', '__', ['ะ']]\n",
      "HYP2 aligned:  [['น'], ['้'], ['า'], ['า']]\n",
      "ms aligned:  [['น'], '__', '__', ['ะ']]\n",
      "Hyp จ้า System จ้า Ref จ๊ะ\n",
      "[['จ'], ['้'], ['า']]\n",
      "[['จ'], ['้'], ['า']]\n",
      "[['จ'], ['๊'], ['ะ']]\n",
      "\n",
      "HYP1 aligned:  [['จ'], ['้'], ['า']]\n",
      "ms aligned:  [['จ'], ['๊'], ['ะ']]\n",
      "HYP2 aligned:  [['จ'], ['้'], ['า']]\n",
      "ms aligned:  [['จ'], ['๊'], ['ะ']]\n",
      "Hyp หน้าเอ็นดู System หน้าเอ็นดู Ref น่าเอ็นดู\n",
      "[['ห'], ['น'], ['้'], ['า'], ['เ'], ['อ'], ['็'], ['น'], ['ด'], ['ู']]\n",
      "[['ห'], ['น'], ['้'], ['า'], ['เ'], ['อ'], ['็'], ['น'], ['ด'], ['ู']]\n",
      "[['น'], ['่'], ['า'], ['เ'], ['อ'], ['็'], ['น'], ['ด'], ['ู']]\n",
      "\n",
      "HYP1 aligned:  [['ห'], ['น'], ['้'], ['า'], ['เ'], ['อ'], ['็'], ['น'], ['ด'], ['ู']]\n",
      "ms aligned:  ['__', ['น'], ['่'], ['า'], ['เ'], ['อ'], ['็'], ['น'], ['ด'], ['ู']]\n",
      "HYP2 aligned:  [['ห'], ['น'], ['้'], ['า'], ['เ'], ['อ'], ['็'], ['น'], ['ด'], ['ู']]\n",
      "ms aligned:  ['__', ['น'], ['่'], ['า'], ['เ'], ['อ'], ['็'], ['น'], ['ด'], ['ู']]\n",
      "Hyp รูกกกกกกก System กกกกกกกก Ref ลูก\n",
      "[['ร'], ['ู'], ['ก'], ['ก'], ['ก'], ['ก'], ['ก'], ['ก'], ['ก']]\n",
      "[['ก'], ['ก'], ['ก'], ['ก'], ['ก'], ['ก'], ['ก'], ['ก']]\n",
      "[['ล'], ['ู'], ['ก']]\n",
      "\n",
      "HYP1 aligned:  [['ร'], ['ู'], ['ก'], ['ก'], ['ก'], ['ก'], ['ก'], ['ก'], ['ก']]\n",
      "ms aligned:  [['ล'], ['ู'], '__', '__', '__', '__', '__', '__', ['ก']]\n",
      "HYP2 aligned:  [['ก'], ['ก'], ['ก'], ['ก'], ['ก'], ['ก'], ['ก'], ['ก']]\n",
      "ms aligned:  ['__', '__', '__', '__', '__', ['ล'], ['ู'], ['ก']]\n",
      "Hyp มะนุด System สะดุด Ref มนุษย์\n",
      "[['ม'], ['ะ'], ['น'], ['ุ'], ['ด']]\n",
      "[['ส'], ['ะ'], ['ด'], ['ุ'], ['ด']]\n",
      "[['ม'], ['น'], ['ุ'], ['ษ'], ['ย'], ['์']]\n",
      "\n",
      "HYP1 aligned:  [['ม'], ['ะ'], ['น'], ['ุ'], '__', '__', ['ด']]\n",
      "ms aligned:  [['ม'], '__', ['น'], ['ุ'], ['ษ'], ['ย'], ['์']]\n",
      "HYP2 aligned:  ['__', ['ส'], ['ะ'], ['ด'], ['ุ'], ['ด']]\n",
      "ms aligned:  [['ม'], ['น'], ['ุ'], ['ษ'], ['ย'], ['์']]\n",
      "Hyp ยุ่ System ยุ่ Ref อยู่\n",
      "[['ย'], ['ุ'], ['่']]\n",
      "[['ย'], ['ุ'], ['่']]\n",
      "[['อ'], ['ย'], ['ู'], ['่']]\n",
      "\n",
      "HYP1 aligned:  ['__', ['ย'], ['ุ'], ['่']]\n",
      "ms aligned:  [['อ'], ['ย'], ['ู'], ['่']]\n",
      "HYP2 aligned:  ['__', ['ย'], ['ุ'], ['่']]\n",
      "ms aligned:  [['อ'], ['ย'], ['ู'], ['่']]\n",
      "Hyp ขอบคุน System ขอบคุณ Ref ขอบคุณ\n",
      "[['ข'], ['อ'], ['บ'], ['ค'], ['ุ'], ['น']]\n",
      "[['ข'], ['อ'], ['บ'], ['ค'], ['ุ'], ['ณ']]\n",
      "[['ข'], ['อ'], ['บ'], ['ค'], ['ุ'], ['ณ']]\n",
      "\n",
      "HYP1 aligned:  [['ข'], ['อ'], ['บ'], ['ค'], ['ุ'], ['น']]\n",
      "ms aligned:  [['ข'], ['อ'], ['บ'], ['ค'], ['ุ'], ['ณ']]\n",
      "HYP2 aligned:  [['ข'], ['อ'], ['บ'], ['ค'], ['ุ'], ['ณ']]\n",
      "ms aligned:  [['ข'], ['อ'], ['บ'], ['ค'], ['ุ'], ['ณ']]\n",
      "Hyp นะค่ะ System ค่ะ Ref นะคะ\n",
      "[['น'], ['ะ'], ['ค'], ['่'], ['ะ']]\n",
      "[['ค'], ['่'], ['ะ']]\n",
      "[['น'], ['ะ'], ['ค'], ['ะ']]\n",
      "\n",
      "HYP1 aligned:  [['น'], ['ะ'], ['ค'], ['่'], ['ะ']]\n",
      "ms aligned:  [['น'], ['ะ'], ['ค'], '__', ['ะ']]\n",
      "HYP2 aligned:  ['__', ['ค'], ['่'], ['ะ']]\n",
      "ms aligned:  [['น'], ['ะ'], ['ค'], ['ะ']]\n",
      "Hyp เจ้างพาภ System เจ้างพาภ Ref เจ้าภาพ\n",
      "[['เ'], ['จ'], ['้'], ['า'], ['ง'], ['พ'], ['า'], ['ภ']]\n",
      "[['เ'], ['จ'], ['้'], ['า'], ['ง'], ['พ'], ['า'], ['ภ']]\n",
      "[['เ'], ['จ'], ['้'], ['า'], ['ภ'], ['า'], ['พ']]\n",
      "\n",
      "HYP1 aligned:  [['เ'], ['จ'], ['้'], ['า'], ['ง'], ['พ'], ['า'], ['ภ']]\n",
      "ms aligned:  [['เ'], ['จ'], ['้'], ['า'], '__', ['ภ'], ['า'], ['พ']]\n",
      "HYP2 aligned:  [['เ'], ['จ'], ['้'], ['า'], ['ง'], ['พ'], ['า'], ['ภ']]\n",
      "ms aligned:  [['เ'], ['จ'], ['้'], ['า'], '__', ['ภ'], ['า'], ['พ']]\n",
      "Hyp ฝั่ง System ฝั่ง Ref ฝัง\n",
      "[['ฝ'], ['ั'], ['่'], ['ง']]\n",
      "[['ฝ'], ['ั'], ['่'], ['ง']]\n",
      "[['ฝ'], ['ั'], ['ง']]\n",
      "\n",
      "HYP1 aligned:  [['ฝ'], ['ั'], ['่'], ['ง']]\n",
      "ms aligned:  [['ฝ'], ['ั'], '__', ['ง']]\n",
      "HYP2 aligned:  [['ฝ'], ['ั'], ['่'], ['ง']]\n",
      "ms aligned:  [['ฝ'], ['ั'], '__', ['ง']]\n",
      "Hyp สายพัน System สารพัน Ref สายพันธุ์\n",
      "[['ส'], ['า'], ['ย'], ['พ'], ['ั'], ['น']]\n",
      "[['ส'], ['า'], ['ร'], ['พ'], ['ั'], ['น']]\n",
      "[['ส'], ['า'], ['ย'], ['พ'], ['ั'], ['น'], ['ธ'], ['ุ'], ['์']]\n",
      "\n",
      "HYP1 aligned:  [['ส'], ['า'], ['ย'], ['พ'], ['ั'], ['น'], '__', '__', '__']\n",
      "ms aligned:  [['ส'], ['า'], ['ย'], ['พ'], ['ั'], ['น'], ['ธ'], ['ุ'], ['์']]\n",
      "HYP2 aligned:  [['ส'], ['า'], ['ร'], ['พ'], ['ั'], ['น'], '__', '__', '__']\n",
      "ms aligned:  [['ส'], ['า'], ['ย'], ['พ'], ['ั'], ['น'], ['ธ'], ['ุ'], ['์']]\n",
      "Hyp เลยยย System เลยยย Ref เลย\n",
      "[['เ'], ['ล'], ['ย'], ['ย'], ['ย']]\n",
      "[['เ'], ['ล'], ['ย'], ['ย'], ['ย']]\n",
      "[['เ'], ['ล'], ['ย']]\n",
      "\n",
      "HYP1 aligned:  [['เ'], ['ล'], ['ย'], ['ย'], ['ย']]\n",
      "ms aligned:  [['เ'], ['ล'], '__', '__', ['ย']]\n",
      "HYP2 aligned:  [['เ'], ['ล'], ['ย'], ['ย'], ['ย']]\n",
      "ms aligned:  [['เ'], ['ล'], '__', '__', ['ย']]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hyp มากกก System มาก Ref มาก\n",
      "[['ม'], ['า'], ['ก'], ['ก'], ['ก']]\n",
      "[['ม'], ['า'], ['ก']]\n",
      "[['ม'], ['า'], ['ก']]\n",
      "\n",
      "HYP1 aligned:  [['ม'], ['า'], ['ก'], ['ก'], ['ก']]\n",
      "ms aligned:  [['ม'], ['า'], '__', '__', ['ก']]\n",
      "HYP2 aligned:  [['ม'], ['า'], ['ก']]\n",
      "ms aligned:  [['ม'], ['า'], ['ก']]\n",
      "Hyp มะวาน System หวาน Ref เมื่อวาน\n",
      "[['ม'], ['ะ'], ['ว'], ['า'], ['น']]\n",
      "[['ห'], ['ว'], ['า'], ['น']]\n",
      "[['เ'], ['ม'], ['ื'], ['่'], ['อ'], ['ว'], ['า'], ['น']]\n",
      "\n",
      "HYP1 aligned:  ['__', ['ม'], '__', '__', ['ะ'], ['ว'], ['า'], ['น']]\n",
      "ms aligned:  [['เ'], ['ม'], ['ื'], ['่'], ['อ'], ['ว'], ['า'], ['น']]\n",
      "HYP2 aligned:  ['__', '__', '__', '__', ['ห'], ['ว'], ['า'], ['น']]\n",
      "ms aligned:  [['เ'], ['ม'], ['ื'], ['่'], ['อ'], ['ว'], ['า'], ['น']]\n",
      "Hyp เอ๊ยยยยยย System เจ๊ยยยยย Ref เอ๊ย\n",
      "[['เ'], ['อ'], ['๊'], ['ย'], ['ย'], ['ย'], ['ย'], ['ย'], ['ย']]\n",
      "[['เ'], ['จ'], ['๊'], ['ย'], ['ย'], ['ย'], ['ย'], ['ย']]\n",
      "[['เ'], ['อ'], ['๊'], ['ย']]\n",
      "\n",
      "HYP1 aligned:  [['เ'], ['อ'], ['๊'], ['ย'], ['ย'], ['ย'], ['ย'], ['ย'], ['ย']]\n",
      "ms aligned:  [['เ'], ['อ'], ['๊'], '__', '__', '__', '__', '__', ['ย']]\n",
      "HYP2 aligned:  [['เ'], ['จ'], ['๊'], ['ย'], ['ย'], ['ย'], ['ย'], ['ย']]\n",
      "ms aligned:  [['เ'], ['อ'], ['๊'], '__', '__', '__', '__', ['ย']]\n",
      "Hyp คือออออออ System ออออออออ Ref คือ\n",
      "[['ค'], ['ื'], ['อ'], ['อ'], ['อ'], ['อ'], ['อ'], ['อ'], ['อ']]\n",
      "[['อ'], ['อ'], ['อ'], ['อ'], ['อ'], ['อ'], ['อ'], ['อ']]\n",
      "[['ค'], ['ื'], ['อ']]\n",
      "\n",
      "HYP1 aligned:  [['ค'], ['ื'], ['อ'], ['อ'], ['อ'], ['อ'], ['อ'], ['อ'], ['อ']]\n",
      "ms aligned:  [['ค'], ['ื'], '__', '__', '__', '__', '__', '__', ['อ']]\n",
      "HYP2 aligned:  [['อ'], ['อ'], ['อ'], ['อ'], ['อ'], ['อ'], ['อ'], ['อ']]\n",
      "ms aligned:  ['__', '__', '__', '__', '__', ['ค'], ['ื'], ['อ']]\n",
      "Hyp เล้ยยยยยยยยย System เล้ยยยยยยยยย Ref เลย\n",
      "[['เ'], ['ล'], ['้'], ['ย'], ['ย'], ['ย'], ['ย'], ['ย'], ['ย'], ['ย'], ['ย'], ['ย']]\n",
      "[['เ'], ['ล'], ['้'], ['ย'], ['ย'], ['ย'], ['ย'], ['ย'], ['ย'], ['ย'], ['ย'], ['ย']]\n",
      "[['เ'], ['ล'], ['ย']]\n",
      "\n",
      "HYP1 aligned:  [['เ'], ['ล'], ['้'], ['ย'], ['ย'], ['ย'], ['ย'], ['ย'], ['ย'], ['ย'], ['ย'], ['ย']]\n",
      "ms aligned:  [['เ'], ['ล'], '__', '__', '__', '__', '__', '__', '__', '__', '__', ['ย']]\n",
      "HYP2 aligned:  [['เ'], ['ล'], ['้'], ['ย'], ['ย'], ['ย'], ['ย'], ['ย'], ['ย'], ['ย'], ['ย'], ['ย']]\n",
      "ms aligned:  [['เ'], ['ล'], '__', '__', '__', '__', '__', '__', '__', '__', '__', ['ย']]\n",
      "Hyp มากกกกกกกก System กกกกกกกก Ref มาก\n",
      "[['ม'], ['า'], ['ก'], ['ก'], ['ก'], ['ก'], ['ก'], ['ก'], ['ก'], ['ก']]\n",
      "[['ก'], ['ก'], ['ก'], ['ก'], ['ก'], ['ก'], ['ก'], ['ก']]\n",
      "[['ม'], ['า'], ['ก']]\n",
      "\n",
      "HYP1 aligned:  [['ม'], ['า'], ['ก'], ['ก'], ['ก'], ['ก'], ['ก'], ['ก'], ['ก'], ['ก']]\n",
      "ms aligned:  [['ม'], ['า'], '__', '__', '__', '__', '__', '__', '__', ['ก']]\n",
      "HYP2 aligned:  [['ก'], ['ก'], ['ก'], ['ก'], ['ก'], ['ก'], ['ก'], ['ก']]\n",
      "ms aligned:  ['__', '__', '__', '__', '__', ['ม'], ['า'], ['ก']]\n",
      "Hyp จ้าาาาา System จ้าาาาา Ref จ๊ะ\n",
      "[['จ'], ['้'], ['า'], ['า'], ['า'], ['า'], ['า']]\n",
      "[['จ'], ['้'], ['า'], ['า'], ['า'], ['า'], ['า']]\n",
      "[['จ'], ['๊'], ['ะ']]\n",
      "\n",
      "HYP1 aligned:  [['จ'], ['้'], ['า'], ['า'], ['า'], ['า'], ['า']]\n",
      "ms aligned:  [['จ'], '__', '__', '__', '__', ['๊'], ['ะ']]\n",
      "HYP2 aligned:  [['จ'], ['้'], ['า'], ['า'], ['า'], ['า'], ['า']]\n",
      "ms aligned:  [['จ'], '__', '__', '__', '__', ['๊'], ['ะ']]\n",
      "Hyp ซิ System ซิ Ref สิ\n",
      "[['ซ'], ['ิ']]\n",
      "[['ซ'], ['ิ']]\n",
      "[['ส'], ['ิ']]\n",
      "\n",
      "HYP1 aligned:  [['ซ'], ['ิ']]\n",
      "ms aligned:  [['ส'], ['ิ']]\n",
      "HYP2 aligned:  [['ซ'], ['ิ']]\n",
      "ms aligned:  [['ส'], ['ิ']]\n",
      "Hyp คับ System คับ Ref ครับ\n",
      "[['ค'], ['ั'], ['บ']]\n",
      "[['ค'], ['ั'], ['บ']]\n",
      "[['ค'], ['ร'], ['ั'], ['บ']]\n",
      "\n",
      "HYP1 aligned:  [['ค'], '__', ['ั'], ['บ']]\n",
      "ms aligned:  [['ค'], ['ร'], ['ั'], ['บ']]\n",
      "HYP2 aligned:  [['ค'], '__', ['ั'], ['บ']]\n",
      "ms aligned:  [['ค'], ['ร'], ['ั'], ['บ']]\n",
      "Hyp อนุญาติ System อนุญาต Ref อนุญาต\n",
      "[['อ'], ['น'], ['ุ'], ['ญ'], ['า'], ['ต'], ['ิ']]\n",
      "[['อ'], ['น'], ['ุ'], ['ญ'], ['า'], ['ต']]\n",
      "[['อ'], ['น'], ['ุ'], ['ญ'], ['า'], ['ต']]\n",
      "\n",
      "HYP1 aligned:  [['อ'], ['น'], ['ุ'], ['ญ'], ['า'], ['ต'], ['ิ']]\n",
      "ms aligned:  [['อ'], ['น'], ['ุ'], ['ญ'], ['า'], ['ต'], '__']\n",
      "HYP2 aligned:  [['อ'], ['น'], ['ุ'], ['ญ'], ['า'], ['ต']]\n",
      "ms aligned:  [['อ'], ['น'], ['ุ'], ['ญ'], ['า'], ['ต']]\n",
      "Hyp ครัฟ System ครับ Ref ครับ\n",
      "[['ค'], ['ร'], ['ั'], ['ฟ']]\n",
      "[['ค'], ['ร'], ['ั'], ['บ']]\n",
      "[['ค'], ['ร'], ['ั'], ['บ']]\n",
      "\n",
      "HYP1 aligned:  [['ค'], ['ร'], ['ั'], ['ฟ']]\n",
      "ms aligned:  [['ค'], ['ร'], ['ั'], ['บ']]\n",
      "HYP2 aligned:  [['ค'], ['ร'], ['ั'], ['บ']]\n",
      "ms aligned:  [['ค'], ['ร'], ['ั'], ['บ']]\n",
      "Hyp แกรน System แกรน Ref แกร็น\n",
      "[['แ'], ['ก'], ['ร'], ['น']]\n",
      "[['แ'], ['ก'], ['ร'], ['น']]\n",
      "[['แ'], ['ก'], ['ร'], ['็'], ['น']]\n",
      "\n",
      "HYP1 aligned:  [['แ'], ['ก'], ['ร'], '__', ['น']]\n",
      "ms aligned:  [['แ'], ['ก'], ['ร'], ['็'], ['น']]\n",
      "HYP2 aligned:  [['แ'], ['ก'], ['ร'], '__', ['น']]\n",
      "ms aligned:  [['แ'], ['ก'], ['ร'], ['็'], ['น']]\n",
      "Hyp มังสาวิรัติ System มังสวิรัติ Ref มังสวิรัติ\n",
      "[['ม'], ['ั'], ['ง'], ['ส'], ['า'], ['ว'], ['ิ'], ['ร'], ['ั'], ['ต'], ['ิ']]\n",
      "[['ม'], ['ั'], ['ง'], ['ส'], ['ว'], ['ิ'], ['ร'], ['ั'], ['ต'], ['ิ']]\n",
      "[['ม'], ['ั'], ['ง'], ['ส'], ['ว'], ['ิ'], ['ร'], ['ั'], ['ต'], ['ิ']]\n",
      "\n",
      "HYP1 aligned:  [['ม'], ['ั'], ['ง'], ['ส'], ['า'], ['ว'], ['ิ'], ['ร'], ['ั'], ['ต'], ['ิ']]\n",
      "ms aligned:  [['ม'], ['ั'], ['ง'], ['ส'], '__', ['ว'], ['ิ'], ['ร'], ['ั'], ['ต'], ['ิ']]\n",
      "HYP2 aligned:  [['ม'], ['ั'], ['ง'], ['ส'], ['ว'], ['ิ'], ['ร'], ['ั'], ['ต'], ['ิ']]\n",
      "ms aligned:  [['ม'], ['ั'], ['ง'], ['ส'], ['ว'], ['ิ'], ['ร'], ['ั'], ['ต'], ['ิ']]\n",
      "Hyp แข๊งแรง System แข็งแรง Ref แข็งแรง\n",
      "[['แ'], ['ข'], ['๊'], ['ง'], ['แ'], ['ร'], ['ง']]\n",
      "[['แ'], ['ข'], ['็'], ['ง'], ['แ'], ['ร'], ['ง']]\n",
      "[['แ'], ['ข'], ['็'], ['ง'], ['แ'], ['ร'], ['ง']]\n",
      "\n",
      "HYP1 aligned:  [['แ'], ['ข'], ['๊'], ['ง'], ['แ'], ['ร'], ['ง']]\n",
      "ms aligned:  [['แ'], ['ข'], ['็'], ['ง'], ['แ'], ['ร'], ['ง']]\n",
      "HYP2 aligned:  [['แ'], ['ข'], ['็'], ['ง'], ['แ'], ['ร'], ['ง']]\n",
      "ms aligned:  [['แ'], ['ข'], ['็'], ['ง'], ['แ'], ['ร'], ['ง']]\n",
      "Error rate for system 1: 0.5916230366492147\n",
      "Error rate for system 2: 0.612565445026178\n"
     ]
    }
   ],
   "source": [
    "from pythainlp.spell import correct\n",
    "\n",
    "def align_diff(hyp_aligned, ref_aligned): \n",
    "\n",
    "    n_errors = 0; ref_length = 0\n",
    "    for i in range(len(hyp_aligned)):\n",
    "        if hyp_aligned[i] != ref_aligned[i]: n_errors += 1\n",
    "        if ref_aligned[i] != '__': ref_length += 1\n",
    "    return n_errors, ref_length\n",
    "\n",
    "def align(x, y):\n",
    "    x = ['#'] + x; y = ['#'] + y\n",
    "    m = len(x); n = len(y)\n",
    "    C = [[[0, None]] * n for i in range(m)]\n",
    "    for j in range(1, n): C[0][j] = [C[0][j-1][0]+1, (0, j-1)]\n",
    "    for i in range(1, m): C[i][0] = [C[i-1][0][0]+1, (i-1, 0)]\n",
    "    for i in range(1, m):\n",
    "        for j in range(1, n):\n",
    "            C[i][j] = min([[C[i-1][j-1][0]+int(x[i]!=y[j]), (i-1, j-1)],\n",
    "                           [C[i-1][j][0]+1, (i-1, j)],\n",
    "                           [C[i][j-1][0]+1, (i, j-1)]])\n",
    "    crumbs = [(m-1, n-1)]\n",
    "    while crumbs[-1] != (0, 0):\n",
    "        i, j = crumbs[-1]\n",
    "        crumbs.append(C[i][j][1])\n",
    "    crumbs.reverse()\n",
    "    xa = []; ya = []\n",
    "    for t in range(1, len(crumbs)):\n",
    "        pi, pj = crumbs[t-1]\n",
    "        i, j = crumbs[t]\n",
    "        if pi == i: xa.append('__')\n",
    "        elif pi != i: xa.append(x[i])\n",
    "        if pj == j: ya.append('__')\n",
    "        elif pj != j: ya.append(y[j])\n",
    "    return xa, ya\n",
    "\n",
    "slangs = [\n",
    "'โล๊ะ','น่าจ้า',\n",
    "'น่า','จ้าาา',\n",
    "'เปน','โรงบาล',\n",
    "'จิง','พักก๊อนนน',\n",
    "'ขี้เกียด','น้าาาาา',\n",
    "'ไฟแนน','ใด้',\n",
    "'ใหน','รุ้','ค้าบบบบ',\n",
    " 'น้าา',\n",
    "'จ้า','หน้าเอ็นดู',\n",
    "'รูกกกกกกก','มะนุด',\n",
    "'ยุ่','ขอบคุน',\n",
    "'นะค่ะ','เจ้างพาภ',\n",
    "'ฝั่ง','สายพัน',\n",
    "'เลยยย','มากกก',\n",
    "'มะวาน','เอ๊ยยยยยย',\n",
    "'คือออออออ','เล้ยยยยยยยยย',\n",
    "'มากกกกกกกก','จ้าาาาา',\n",
    "'ซิ','คับ',\n",
    "'อนุญาติ','ครัฟ',\n",
    "'แกรน','มังสาวิรัติ',\n",
    "'แข๊งแรง'\n",
    "]\n",
    "\n",
    "with open ('slangs_corrected') as f:\n",
    "    p = f.readlines()\n",
    "    master = [o.rstrip('\\n') for o in p]\n",
    "    \n",
    "# hyp = 'ขี้เกียด'\n",
    "# ref = 'ขี้เกียจ'\n",
    "# x = [i.split() for i in hyp]\n",
    "# y = [b.split() for b in ref]\n",
    "\n",
    "# ha, ra = align(x,y)\n",
    "\n",
    "tne1 = 0; tne2 = 0; trl = 0\n",
    "for i in range(len(slangs)): \n",
    "   \n",
    "    corrected = correct(slangs[i])\n",
    "    print('Hyp',slangs[i], 'System', corrected, 'Ref', master[i])\n",
    "    x = [a.split() for a in slangs[i]]\n",
    "    y = [b.split() for b in corrected]\n",
    "    z = [c.split() for c in master[i]]\n",
    "    print(x)\n",
    "    print(y)\n",
    "    print(z)\n",
    "    print('')  \n",
    "    hyp1, ms1 = align(x,z) # pair 1 to evaluate slangs\n",
    "    hyp2, ms2 = align(y,z) # pair 2 to evaluate corrected\n",
    "#diff = diff(hyp,rf)  #\n",
    "#print('Diff', diff)\n",
    "    print('HYP1 aligned: ',hyp1)\n",
    "    print('ms aligned: ', ms1)\n",
    "    ne1, rl = align_diff(hyp1, ms1)\n",
    "    print('HYP2 aligned: ', hyp2)\n",
    "    print('ms aligned: ', ms2)\n",
    "    ne2, rl = align_diff(hyp2, ms2)\n",
    "    tne1 += ne1; tne2 += ne2; trl += rl\n",
    "\n",
    "print('Error rate for system 1:', tne1/trl) #0.5916230366492147\n",
    "print('Error rate for system 2:', tne2/trl) #0.612565445026178\n",
    "# To report the final error rate:\n",
    "# divide \n",
    "# total number of all errors (different positions)\n",
    "# by\n",
    "# sum of all reference lengths (make sure to ignore underscores in measuring reference lengths)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
